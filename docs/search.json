[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "직업환경연구방법",
    "section": "",
    "text": "소개\n직업병 역학조사 연구에 필요한 방법론을 저장하는 곳입니다.\njinha@dspubs.org",
    "crumbs": [
      "소개"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  개요",
    "section": "",
    "text": "연세대학교 산업보건 연구소 교육 내용을 위주로 구성되어 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>개요</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "MetaAnalysis.html",
    "href": "MetaAnalysis.html",
    "title": "2  메타분석",
    "section": "",
    "text": "2.1 메타 분석의 이해\n참고자료: R에서 메타 분석 수행하기\nhttps://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>메타분석</span>"
    ]
  },
  {
    "objectID": "MetaAnalysis.html#메타분석이란-무엇인가",
    "href": "MetaAnalysis.html#메타분석이란-무엇인가",
    "title": "2  메타분석",
    "section": "",
    "text": "정의: 메타분석은 “분석의 분석”으로, 개별 연구를 단위로 삼아 모든 관련 증거를 종합하고 해석하는 방법 (Glass, 1976).\n목표: 특정 연구 질문에 대한 증거를 통합하여 요약하고, 정량적 결과를 하나의 수치로 추정 (예: 약물 효과, 질병 유병률 등).\n유형 비교:\n\n전통적/서술적 리뷰: 전문가가 주관적으로 연구를 선택하고 결론을 도출, 편향 가능성 있음.\n체계적 리뷰: 투명한 규칙으로 증거를 요약, 모든 연구 포함 및 타당성 평가.\n메타분석: 체계적 리뷰의 고급 형태로, 정량적 통합이 특징.\n개인 참여자 데이터(IPD) 메타분석: 개별 데이터를 수집해 분석, 세부 변수 탐색 가능하나 데이터 접근 어려움으로 제한적.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>메타분석</span>"
    ]
  },
  {
    "objectID": "MetaAnalysis.html#메타분석의-역사-터무니없는-실습",
    "href": "MetaAnalysis.html#메타분석의-역사-터무니없는-실습",
    "title": "2  메타분석",
    "section": "2.2 1.2 메타분석의 역사: “터무니없는 실습”?",
    "text": "2.2 1.2 메타분석의 역사: “터무니없는 실습”?\n\n기원: 20세기 초 Pearson과 Fisher가 통계적 종합 시도. Glass가 “메타분석”이라는 용어를 만들며 발전 (1976).\n주요 사건:\n\nEysenck(1952)이 심리치료 효과 부정 → Smith와 Glass(1977)가 메타분석으로 반박 (효과 크기 0.68).\nEysenck은 이를 “학문적 포기”라 비판했으나, 이후 메타분석 보편화.\n\n발전:\n\nHunter와 Schmidt: 측정 오류 보정 기법.\n의학: Elwood와 Cochrane, aspirin 효과 입증.\nDerSimonian과 Laird: 무작위효과 모델 개발 (1986).\n\n조직: Cochrane(1993)와 Campbell Collaboration이 표준화 및 적용 주도.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>메타분석</span>"
    ]
  },
  {
    "objectID": "MetaAnalysis.html#메타분석의-함정-사과와-오렌지",
    "href": "MetaAnalysis.html#메타분석의-함정-사과와-오렌지",
    "title": "2  메타분석",
    "section": "2.3 1.3 메타분석의 함정: 사과와 오렌지?",
    "text": "2.3 1.3 메타분석의 함정: 사과와 오렌지?\n\n문제점:\n\n사과와 오렌지: 연구 간 차이로 의미 없는 통합 가능. 연구 질문에 따라 적절히 조정 필요.\n쓰레기 입력, 쓰레기 출력: 원 연구의 질이 낮으면 결과도 신뢰도 낮음.\n서랍 속 문제: 출판되지 않은 연구(특히 부정적 결과) 누락으로 출판 비뚤림 발생.\n연구자 의제: 연구자의 주관적 선택이 결과에 영향.\n\n해결책: 명확한 연구 질문, 사전 등록, 투명한 방법론으로 완화 가능.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>메타분석</span>"
    ]
  },
  {
    "objectID": "MetaAnalysis.html#문제-정의-연구-검색-및-코딩",
    "href": "MetaAnalysis.html#문제-정의-연구-검색-및-코딩",
    "title": "2  메타분석",
    "section": "2.4 1.4 문제 정의, 연구 검색 및 코딩",
    "text": "2.4 1.4 문제 정의, 연구 검색 및 코딩\n\n연구 질문 정의:\n\nFINER 기준: 실현 가능(Feasible), 흥미로운(Interesting), 새로운(Novel), 윤리적인(Ethical), 관련성 있는(Relevant).\nPICO 프레임워크: 인구(Population), 중재(Intervention), 비교(Comparison), 결과(Outcome).\n\n연구 검색:\n\n출처: 리뷰, 참고문헌, 데이터베이스 (PubMed, PsycInfo 등), 회색 문헌.\n검색 문자열: AND/OR 연산자, 와일드카드 활용 (예: “sociolog*“).\n\n연구 선택:\n\n3단계: 중복 제거 → 제목/초록 검토 → 전체 문헌 검토.\n이중 검토(Double-Screening): 오류 최소화.\n\n데이터 추출 및 코딩:\n\n연구 특성 (저자, 연도, 표본 크기 등), 효과 크기 데이터, 연구 질/비뚤림 위험.\nCochrane Risk of Bias Tool 등으로 질 평가.\n\n사전 등록: 분석 계획 공개 (OSF, PROSPERO)로 투명성 확보.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>메타분석</span>"
    ]
  },
  {
    "objectID": "MetaAnalysis.html#질문과-답변",
    "href": "MetaAnalysis.html#질문과-답변",
    "title": "2  메타분석",
    "section": "2.5 1.5 질문과 답변",
    "text": "2.5 1.5 질문과 답변\n\n메타분석 정의, 창시자 (예: Glass), 주요 문제점, 좋은 연구 질문의 특성, PICO 추출법, 검색 출처, 질 vs. 비뚤림 차이 등 학습 점검.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>메타분석</span>"
    ]
  },
  {
    "objectID": "MetaAnalysis.html#메타-분석의-이해",
    "href": "MetaAnalysis.html#메타-분석의-이해",
    "title": "2  메타분석",
    "section": "",
    "text": "2.1.1 메타분석이란 무엇인가?\n\n정의: 메타분석은 “분석의 분석”으로, 개별 연구를 단위로 삼아 모든 관련 증거를 종합하고 해석하는 방법 (Glass, 1976).\n목표: 특정 연구 질문에 대한 증거를 통합하여 요약하고, 정량적 결과를 하나의 수치로 추정 (예: 약물 효과, 질병 유병률 등).\n유형 비교:\n\n전통적/서술적 리뷰: 전문가가 주관적으로 연구를 선택하고 결론을 도출, 편향 가능성 있음.\n체계적 리뷰: 투명한 규칙으로 증거를 요약, 모든 연구 포함 및 타당성 평가.\n메타분석: 체계적 리뷰의 고급 형태로, 정량적 통합이 특징.\n개인 참여자 데이터(IPD) 메타분석: 개별 데이터를 수집해 분석, 세부 변수 탐색 가능하나 데이터 접근 어려움으로 제한적.\n\n\n\n\n2.1.2 문제 정의, 연구 검색 및 코딩\n체계적 문헌 고찰의 첫 번째 단계는 연구의 핵심(초점)을 결정하는 가장 우선적이고 중요한 과정입니다 . 좋은 핵심 질문은 체계적 문헌 고찰의 전체 과정을 이끌고, 결국 양질의 연구 결과를 도출하는 데 결정적인 역할을 합니다 . 연구 질문을 설정할 때는 일반적으로 PICO(Patient, Intervention, Comparison, Outcome) 또는 필요에 따라 PICOTS(Patient, Intervention, Comparison, Outcome, Time, Setting) 프레임워크를 활용하여 질문을 구체화합니다 . 연구 질문은 기존의 증거에 의해 결정되어야 할 연구 주제에 대한 결론을 얻기 위해 명확하게 작성되어야 하며 , 그 구성 요소는 연구 디자인과 함께 일차 연구물을 선택할 때 사용될 수 있도록 더욱 정교하게 만들어져야 합니다 . 또한, 연구 질문은 조사하고자 하는 문제를 명확하게 정의하고, 그것이 구체적이고 중요한 문제인지 확인해야 합니다 . 명확하고 집중된 연구 질문을 설정하는 것은 체계적 문헌 고찰의 전체 과정을 안내하는 기본적인 단계입니다. 잘 정의된 질문은 검토의 관련성을 유지하고 특정 지식 격차를 해결하는 데 도움이 됩니다. PICO/PICOTS 프레임워크는 연구 질문의 핵심 요소를 정의하는 구조화된 접근 방식을 제공하여 모든 관련 측면을 고려하도록 보장합니다. 이 프레임워크는 후속 문헌 검색 및 연구 선택 단계에도 도움이 됩니다\n\n연구 질문 정의:\n\nFINER 기준: 실현 가능(Feasible), 흥미로운(Interesting), 새로운(Novel), 윤리적인(Ethical), 관련성 있는(Relevant).\nPICO 프레임워크: 인구(Population), 중재(Intervention), 비교(Comparison), 결과(Outcome).\n\n연구 검색:\n\n출처: 리뷰, 참고문헌, 데이터베이스 (PubMed, PsycInfo 등), 회색 문헌.\n검색 문자열: AND/OR 연산자, 와일드카드 활용 (예: “sociolog*”).\n\n연구 선택:\n\n3단계: 중복 제거 → 제목/초록 검토 → 전체 문헌 검토.\n이중 검토(Double-Screening): 오류 최소화.\n\n데이터 추출 및 코딩:\n\n연구 특성 (저자, 연도, 표본 크기 등), 효과 크기 데이터, 연구 질/비뚤림 위험.\n\n\n\n\n\nCOSI\n\n\nPICO/PICOS 구성:\n\n\n\n\n\n\n\n항목\n세부 내용\n\n\n\n\nP (Population)\n채광, 건설, 석재 가공 등 결정형 유리규산에 노출되는 직업에 종사하는 근로자\n\n\nI (Intervention)\n결정형 유리규산 분진에 직업적으로 노출\n\n\nC (Comparison)\n결정형 유리규산에 노출되지 않거나 노출 수준이 낮은 근로자\n\n\nO (Outcome)\n위암 발생률, 위암 사망률\n\n\nS (Study design)\n코호트 연구, 환자-대조군 연구\n\n\n\n\n\n2.1.3 체계적인 문헌 검색 수행\n문헌 검색 단계에서는 다양하고 포괄적인 자료원을 조사하여 연구 주제와 관련된 모든 연구가 누락되지 않도록 주의해야 합니다. 이를 위해 가능한 다양한 검색 엔진(PubMed, Embase, Web of Science 등)을 활용하는 것이 중요합니다 . 평가를 위한 PICO가 결정되면, 연구 질문에 대한 답을 찾기 위해 이용 가능한 모든 데이터베이스를 사전에 결정하고, 각 데이터베이스의 특성을 고려하여 검색 효율성을 높여야 합니다 . 일반적으로 MEDLINE, EMBASE, Cochrane Library와 같은 주요 데이터베이스를 검색하는 것이 권장됩니다 . 국내 연구를 포함하기 위해서는 KoreaMed, Kmbase, KISS, Riss4U, KISTI 등의 국내 전자 데이터베이스를 활용해야 하며, 검색 시 띄어쓰기 및 한글과 영어를 모두 포함하여 검색하는 것이 중요합니다 . 또한, 전자 데이터베이스 검색뿐만 아니라 수기 검색(hand searching)과 기존에 확보한 문헌들의 참고 문헌 검색을 병행하여 검색의 포괄성을 높여야 합니다 . 출판된 문헌 외에도 임상 시험 등록 데이터베이스를 검색하여 출판되지 않은 결과를 찾는 것도 중요하며 , 연구 주제에 맞는 미출판 문헌 및 회색 문헌을 적극적으로 탐색해야 합니다.\n다양한 데이터베이스와 검색 방법을 활용하는 것은 검색의 포괄성을 극대화하고 출판 편향을 최소화하는 데 매우 중요합니다. 이는 표준 학술 데이터베이스뿐만 아니라 회색 문헌 및 임상 시험 등록소까지 포함합니다. 전자 데이터베이스 검색만으로는 모든 관련 연구를 포착하지 못할 수 있으므로, 수기 검색과 참고 문헌 확인을 병행하는 것이 필요합니다.\n\n\n2.1.4 문헌 선정 및 배제 기준 적용\n검색된 문헌 중에서 체계적 문헌 고찰에 포함할 연구와 제외할 연구를 결정하기 위해 사전에 정의된 명확한 기준을 적용해야 합니다 . 평가를 위한 구체적인 연구 질문에 대한 답을 제공하고, 편향을 제거하기 위해 연구 계획 단계에서 선택 및 배제 기준을 명확하게 명시해야 합니다 . 문헌 선택 기준은 포함 기준과 배제 기준을 모두 제시해야 하며, 연구 목적에 부합하도록 논리적으로 구성되어야 합니다 . 문헌을 선택할 때는 설정된 배제 기준 중 하나라도 해당되면 해당 문헌을 제외해야 하며, 모든 포함 기준을 만족하는 문헌만 포함해야 합니다 . 이 과정의 객관성과 투명성을 확보하기 위해 두 명 이상의 평가자가 독립적으로 문헌을 검토하고 선정해야 합니다 . 평가자들의 의견이 서로 다를 경우에는 평가자 간의 토론이나 제3자의 개입을 통해 합의를 도출해야 합니다 . 제외되는 문헌이 있을 경우, 그 이유를 명확하게 기록해야 합니다 .\n명확하게 정의된 포함 및 배제 기준은 관련성이 높은 연구만을 체계적 문헌 고찰에 포함시켜 편향을 줄이고 연구 결과의 타당성을 높이는 데 필수적입니다. 연구 선택 과정에 최소 두 명의 독립적인 검토자를 참여시키는 것은 객관성을 높이고 개인적인 편향이 결과에 미치는 영향을 줄이는 데 도움이 됩니다. 의견 불일치는 토론이나 제3자의 중재를 통해 해결해야 합니다.\n\n\n2.1.5 자료 추출\n문헌 선정 단계를 거친 후에는 체계적 문헌 고찰에 포함된 각 연구에서 필요한 정보를 체계적으로 추출하고 코딩하는 단계가 진행됩니다 . 자료 추출 형식은 연구 목적과 평가의 틀에 부합하도록 신중하게 개발해야 합니다 . 추출하는 정보는 연구 질문과 직접적으로 관련되어야 하며, 정보의 양은 너무 상세하거나 너무 간결하지 않도록 적절해야 합니다 . 자료 추출 형식은 동일한 표본으로 여러 평가자에 의해 시범적으로 검토되어 개발되어야 합니다 . 자료 추출은 최소한 두 명의 평가자가 독립적으로 수행해야 하며, 추출된 결과는 서로 비교하여 일치 여부를 확인해야 합니다 . 자료 추출 결과에 불일치가 있을 경우, 평가자들은 토론을 통해 상호 합의를 시도해야 하며, 합의에 이르지 못할 경우에는 제3자의 개입이나 전문가의 자문을 구해야 합니다 . 일반적으로 추출되는 정보에는 연구 수행 시기, 출판 연도, 국가 또는 지역, 연구 방법, 노출 특성 및 평가 방법, 질병 정의 방식, 연구 결과 등이 포함됩니다.\n체계적이고 표준화된 자료 추출 방식은 포함된 연구에서 수집된 정보의 정확성과 완전성을 보장하는 데 매우 중요합니다. 이 단계는 후속 분석 및 종합의 기초를 형성합니다. 자료 추출 형식의 내용은 연구 질문에 답하는 데 필요한 모든 관련 정보를 포착하도록 신중하게 고려해야 합니다. 여러 명의 검토자가 참여하면 정확성과 일관성을 확보할 수 있습니다.\n\n\n2.1.6 문헌의 질 평가 및 삐뚤림 위험 분석\n체계적 문헌 고찰에 포함된 각 연구의 방법론적 질을 평가하고 비뚤림 위험을 분석하는 것은 매우 중요한 단계입니다 . 이는 각 논문이 제공하는 정보가 객관적이고 타당한 근거를 제시하는지 확인하기 위한 과정입니다 . 모든 연구는 결과에 영향을 미칠 수 있는 편향의 가능성을 내포하고 있으므로, 문헌의 질을 평가하는 것은 필수적입니다 . 질 평가는 근거를 종합하는 데 필수적인 요소이며, 투명하고 재현 가능한 방식으로 수행되어야 합니다 . 평가 내용은 주로 연구의 방법론적 질에 초점을 맞추며, 결과값 자체의 효과 크기와는 관련이 적습니다 . 연구 설계 유형에 따라 무작위 임상 실험용 질 평가 도구와 비무작위 임상 실험용 질 평가 도구가 구분되어 사용됩니다 . 대표적인 질 평가 도구로는 Newcastle-Ottawa, ROBINS-I, ROBINS-E 등이 있습니다 . 질 평가는 주로 선택 편향, 실행 편향, 탈락 편향, 결과 확인 편향 등 4가지 주요 영역을 평가합니다 . 질 평가 결과는 체계적 문헌 고찰에서 도출된 근거의 수준을 결정하는 데 중요한 역할을 합니다 .\n\n\n2.1.7 자료 분석 및 종합\n선택된 문헌에서 추출한 자료를 바탕으로 연구 질문에 대한 답을 찾기 위해 자료를 분석하고 종합하는 단계입니다 . 자료 종합은 연구의 특성과 결과의 형태에 따라 정성적 또는 정량적 방법으로 수행될 수 있습니다. 정성적 종합은 통계적 방법을 사용하지 않고 관련 연구의 결과를 요약하는 방식으로 이루어집니다 . 반면, 정량적 종합은 메타분석을 통해 이루어지며, 이는 여러 개별 연구의 결과를 통계적으로 결합하여 전반적인 효과 크기를 추정하는 방법입니다 . 메타분석은 두 개 이상의 연구를 대상으로 각 연구 결과에 대한 가중 평균을 계산하는 개념입니다 . 메타분석의 주된 목적은 검정력과 정확성을 높여 개별 연구에서는 찾을 수 없었던 결론에 대한 답을 얻는 것입니다 . 자료 분석 및 종합 단계에서는 연구들 간의 이질성(heterogeneity)을 평가하고 관리하는 것이 중요합니다 . 이질성은 하위집단 분석, 메타 회귀분석, 민감도 분석 등을 통해 탐색하고 설명할 수 있습니다.\n개별 연구의 결과를 종합하는 것은 체계적 문헌 고찰의 핵심입니다. 이는 포함된 연구의 성격과 연구 질문에 따라 서술적(정성적) 종합 또는 메타분석(정량적)을 포함할 수 있습니다. 메타분석은 가능한 경우 여러 연구 결과를 결합하여 통계적 검정력을 높이고 효과 크기에 대한 더 정확한 추정치를 제공합니다. 그러나 연구 간의 이질성을 평가하고 해결하는 것은 통합된 추정치의 타당성을 보장하는 데 중요합니다.\n\n\n2.1.8 결과 제시 및 해석\n자료 분석 및 종합 단계를 거쳐 얻은 결과를 연구의 맥락에 맞게 제시하고 해석하는 것은 체계적 문헌 고찰의 중요한 부분입니다 . 연구 결과는 일반적으로 글, 표, 그림 등의 다양한 형식을 활용하여 명확하게 제시해야 합니다 . 특히 메타분석을 수행한 경우에는 결과를 forest plot을 사용하여 시각적으로 설명하는 것이 일반적입니다 . 결과를 해석할 때는 통계적 유의성뿐만 아니라 임상적 또는 실제적인 의미를 함께 고려해야 합니다 . 또한, 개별 연구들 간에 나타나는 차이의 원인을 규명하거나 설명하는 것이 중요하며 , 근거의 양과 질, 일관성, 그리고 연구 결과의 일반화 가능성 등을 종합적으로 고려하여 최종 결론을 도출해야 합니다.\n결과를 명확하고 간결하게 제시하는 것은 연구 질문과 대상 독자에게 맞게 조정되어야 합니다. 표와 그림, 특히 메타분석의 경우 forest plot과 같은 시각 자료는 결과를 효과적으로 전달하는 데 매우 유용할 수 있습니다. 결과를 해석할 때는 통계적 유의성뿐만 아니라 기존 지식과 연구 대상 집단의 특정 맥락 내에서 결과의 임상적 또는 실제적 의미를 고려해야 합니다.\n\n\n2.1.9 체계적 문헌 고찰 보고서 작성\n체계적 문헌 고찰의 마지막 단계는 연구 과정과 결과를 상세하게 기술하는 보고서를 작성하는 것입니다. 체계적 문헌 고찰 보고서는 일반적으로 원저 논문의 구조와 유사하게 제목, 초록, 서론, 방법론, 결과, 고찰, 참고 문헌 등으로 구성됩니다 . 보고서를 작성할 때는 PRISMA (Preferred Reporting Items for Systematic reviews and Meta-Analyses)와 같은 표준화된 보고 지침을 준수하는 것이 좋습니다 . 방법론 섹션에서는 포함/배제 기준, 연구 제시, 연구 선택 과정, 데이터 추출 방법, 질 평가 방법, 데이터 분석 방법 등을 명확하고 논리적으로 설명해야 합니다 . 결과 섹션에서는 문헌 검색 결과, 포함된 연구의 범위와 특징, 연구의 질 평가 결과, 그리고 결과에 대한 개입의 영향력 등을 상세히 기술해야 합니다 . 고찰 섹션에서는 주요 결과를 요약하고, 연구의 한계점 및 결과의 신뢰성에 대해 논의해야 합니다 . 참고 문헌 목록은 누락 없이 정확하게 작성해야 하며 , 연구 계획서(protocol)를 작성하여 보고서에 첨부하는 것도 권장됩니다.\n\n\n2.1.10 요약\n\n연구 질문 정의:\n\nFINER 기준: 실현 가능(Feasible), 흥미로운(Interesting), 새로운(Novel), 윤리적인(Ethical), 관련성 있는(Relevant).\nPICO 프레임워크: 인구(Population), 중재(Intervention), 비교(Comparison), 결과(Outcome).\n\n연구 검색:\n\n출처: 리뷰, 참고문헌, 데이터베이스 (PubMed, PsycInfo 등), 회색 문헌.\n검색 문자열: AND/OR 연산자, 와일드카드 활용 (예: “sociolog*”).\n\n연구 선택:\n\n3단계: 중복 제거 → 제목/초록 검토 → 전체 문헌 검토.\n이중 검토(Double-Screening): 오류 최소화.\n\n데이터 추출 및 코딩:\n\n연구 특성 (저자, 연도, 표본 크기 등), 효과 크기 데이터, 연구 질/비뚤림 위험.\nCochrane Risk of Bias Tool 등으로 질 평가.\n\n사전 등록: 분석 계획 공개 (OSF, PROSPERO)로 투명성 확보.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>메타분석</span>"
    ]
  },
  {
    "objectID": "MetaAnalysis.html#효과-크기",
    "href": "MetaAnalysis.html#효과-크기",
    "title": "2  메타분석",
    "section": "2.2 효과 크기",
    "text": "2.2 효과 크기\n개요\n\n목적: 메타분석은 여러 연구의 정량적 결과를 요약하며, 이를 위해 효과 크기(effect size)를 계산하고 통합.\n문제: 개별 연구에서는 동일한 척도로 결과 변수를 측정하지만, 메타분석에서는 연구마다 측정 방식이 달라 직접 통합 불가.\n해결: 동일한 의미를 갖는 효과 크기를 선택해 비교 가능하도록 함.\n\n효과 크기의 기준\n\n비교 가능성: 모든 연구에서 동일한 의미를 가짐 (예: 서로 다른 수학 시험 점수 직접 비교 불가).\n계산 가능성: 연구 데이터로부터 계산 가능해야 함.\n신뢰성: 표준오차 계산 가능하고 통계적 통합에 적합해야 함.\n해석 가능성: 연구 질문에 적절하고 이해하기 쉬워야 함 (필요 시 변환 적용).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>메타분석</span>"
    ]
  },
  {
    "objectID": "MetaAnalysis.html#효과-크기-effect-sizes",
    "href": "MetaAnalysis.html#효과-크기-effect-sizes",
    "title": "2  메타분석",
    "section": "2.2 효과 크기 (Effect Sizes)",
    "text": "2.2 효과 크기 (Effect Sizes)\n효과 크기란\n\n효과 크기 는 개별 연구 결과를 표준화하여 비교 가능하게 만드는 수치입니다.\n\n예: 두 집단의 평균 차이(예: 시험 점수)나 사건 발생 비율(예: 치료 성공률)을 정량화.\n\n메타분석의 핵심은 연구 간 차이를 없애고 통합된 결과를 도출하는 데 있으며, 이를 위해 효과 크기가 필수적입니다.\n\n효과 크기 선택 기준 : 효과 크기를 선택할 때는 다음 세 가지를 고려해야 합니다:\n\n비교 가능성: 서로 다른 연구에서 동일한 지표로 변환 가능해야 함.\n계산 가능성: 연구 데이터(예: 평균, 표준편차 등)로 계산할 수 있어야 함.\n신뢰성: 통계적으로 안정적인 결과를 제공해야 함.\n\n주요 유형\n\nCohen’s d: 설명: 연속형 결과(예: 시험 점수, 키)에서 두 집단 간 평균 차이를 표준편차로 나눈 값. \\[     d = \\frac{M_1 - M_2}{\\sqrt{\\frac{SD_1^2 + SD_2^2}{2}}}     \\] 여기서 (M_1, M_2)는 각각 실험군과 대조군의 평균, (SD_1, SD_2)는 표준편차입니다.\n\n\\[     d \\approx 0.2 \\text{(작은 효과)}, \\quad d \\approx 0.5 \\text{(중간 효과)}, \\quad d \\approx 0.8 \\text{(큰 효과)}     \\]\n\n오즈비(Odds Ratio, OR): 이진 결과(예: 질병 발생 여부)에서 사건 발생 확률의 비율.\n\n\\[OR = \\frac{\\frac{a}{b}}{\\frac{c}{d}} = \\frac{a \\cdot d}{b \\cdot c}\\]\n여기서 (a, b)는 실험군의 성공/실패 수, (c, d)는 대조군의 성공/실패 수. 예: 치료군과 대조군의 성공 확률 비교.\n\n위험비(Risk Ratio, RR): 사건 발생 위험의 비율. OR과 비슷하지만 해석이 더 직관적.\n\n\\[RR = \\frac{\\frac{a}{a+b}}{\\frac{c}{c+d}}\\]\n\n상관계수(r): 설명: 두 변수 간 관계 강도를 나타냄.\n\n\\[-1 \\leq r \\leq 1\\] (r &gt; 0)은 양의 상관, (r &lt; 0)은 음의 상관, (r = 0)은 상관 없음\n\n2.2.1 효과 크기 통합 (Pooling Effect Sizes)\n통합의 필요성\n\n효과 크기 통합 은 개별 연구의 결과를 하나로 합쳐 전체적인 효과를 추정합니다.\n이를 통해 연구 간 일관성 여부와 평균 효과를 확인할 수 있습니다.\n결국 평균을 내는데, 각 연구의 가중치를 부여한다. 입니다.\n\n\nif(!require(devtools)) install.packages(\"devtools\")\nif(!require(tidyverse)) install.packages(\"tidyverse\")\nif(!require(meta)) install.packages(\"meta\")\nif(!require(dmetar)) devtools::install_github(\"MathiasHarrer/dmetar\")\nif(!require(esc)) install.packages(\"esc\")\nif(!require(htmlTable)) install.packages(\"htmlTable\")\n\n\nlibrary(dmetar)\nlibrary(esc)\nlibrary(tidyverse)\ndata(SuicidePrevention)\nSuicidePrevention %&gt;% head()\n\n           author n.e mean.e sd.e n.c mean.c sd.c pubyear age_group\n1    Berry et al.  90  14.98 3.29  95  15.54 4.41    2006       gen\n2  DeVries et al.  77  16.21 5.35  69  20.13 7.43    2019     older\n3  Fleming et al.  30   3.01 0.87  30   3.13 1.23    2006       gen\n4    Hunt & Burke  64  19.32 6.41  65  20.22 7.62    2011       gen\n5 McCarthy et al.  50   4.54 2.75  50   5.61 2.66    1997       gen\n6   Meijer et al. 109  15.11 4.63 111  16.46 5.39    2000       gen\n          control\n1             WLC\n2 no intervention\n3 no intervention\n4             WLC\n5             WLC\n6 no intervention\n\n\nSuicidePrevention은 dmetar 패키지에 포함된 예제 데이터셋으로, 자살 예방 프로그램의 효과를 평가한 연구 데이터를 담고 있습니다. 이 데이터는 보통 다음과 같은 열(column)을 포함합니다:\n\nmean.e: 실험군(experimental group)의 평균\nsd.e: 실험군의 표준편차\nn.e: 실험군의 표본 크기\nmean.c: 대조군(control group)의 평균\nsd.c: 대조군의 표준편차\nn.c: 대조군의 표본 크기\nauthor: 연구 저자 또는 연구 이름\n\n\nlibrary(dmetar)\nlibrary(esc)\nlibrary(tidyverse)\n# Load data set from dmetar\ndata(SuicidePrevention)\n\n# Calculate Hedges' g and the Standard Error\n# - We save the study names in \"study\".\n# - We use the pmap_dfr function to calculate the effect size\n#   for each row.\nSP_calc &lt;- pmap_dfr(SuicidePrevention, \n                    function(mean.e, sd.e, n.e, mean.c,\n                             sd.c, n.c, author, ...){\n                      esc_mean_sd(grp1m = mean.e,\n                                  grp1sd = sd.e,\n                                  grp1n = n.e,\n                                  grp2m = mean.c,\n                                  grp2sd = sd.c,\n                                  grp2n = n.c,\n                                  study = author,\n                                  es.type = \"g\") %&gt;%  #d : cohen's d\n                        as.data.frame()}) \n\n# Let us catch a glimpse of the data\n# The data set contains Hedges' g (\"es\") and standard error (\"se\")\nSP_calc %&gt;% head()\n\n            study         es   weight sample.size        se        var\n1    Berry et al. -0.1427945 46.09784         185 0.1472854 0.02169299\n2  DeVries et al. -0.6077093 34.77314         146 0.1695813 0.02875783\n3  Fleming et al. -0.1111797 14.97625          60 0.2584036 0.06677240\n4    Hunt & Burke -0.1269801 32.18243         129 0.1762749 0.03107286\n5 McCarthy et al. -0.3924744 24.52054         100 0.2019459 0.04078214\n6   Meijer et al. -0.2675788 54.50431         220 0.1354517 0.01834717\n       ci.lo        ci.hi measure\n1 -0.4314686  0.145879624       g\n2 -0.9400826 -0.275335960       g\n3 -0.6176413  0.395282029       g\n4 -0.4724727  0.218512440       g\n5 -0.7882811  0.003332195       g\n6 -0.5330593 -0.002098274       g\n\n\n\nSP_calc$w &lt;- 1/SP_calc$se^2\n\n# Then, we use the weights to calculate the pooled effect\npooled_effect &lt;- sum(SP_calc$w*SP_calc$es)/sum(SP_calc$w)\npooled_effect\n\n[1] -0.2311121\n\n\n이미 계산된 값을 가지고 할 경우\n\nThirdWave %&gt;% head()\n\n           Author        TE      seTE RiskOfBias TypeControlGroup\n1     Call et al. 0.7091362 0.2608202       high              WLC\n2 Cavanagh et al. 0.3548641 0.1963624        low              WLC\n3   DanitzOrsillo 1.7911700 0.3455692       high              WLC\n4  de Vibe et al. 0.1824552 0.1177874        low  no intervention\n5  Frazier et al. 0.4218509 0.1448128        low information only\n6  Frogeli et al. 0.6300000 0.1960000        low  no intervention\n  InterventionDuration InterventionType ModeOfDelivery\n1                short      mindfulness          group\n2                short      mindfulness         online\n3                short              ACT          group\n4                short      mindfulness          group\n5                short              PCI         online\n6                short              ACT          group\n\n\n\nm.gen &lt;- metagen(TE = TE,\n                 seTE = seTE,\n                 studlab = Author,\n                 data = ThirdWave,\n                 sm = \"SMD\", #standard mean difference\n                 fixed = FALSE,\n                 random = TRUE,\n                 method.tau = \"REML\",\n                 method.random.ci = \"HK\",\n                 title = \"Third Wave Psychotherapies\")\n\nWarning: Use argument 'common' instead of 'fixed' (deprecated).\n\nsummary(m.gen)\n\nReview:     Third Wave Psychotherapies\n\n                          SMD            95%-CI %W(random)\nCall et al.            0.7091 [ 0.1979; 1.2203]        5.0\nCavanagh et al.        0.3549 [-0.0300; 0.7397]        6.3\nDanitzOrsillo          1.7912 [ 1.1139; 2.4685]        3.8\nde Vibe et al.         0.1825 [-0.0484; 0.4133]        7.9\nFrazier et al.         0.4219 [ 0.1380; 0.7057]        7.3\nFrogeli et al.         0.6300 [ 0.2458; 1.0142]        6.3\nGallego et al.         0.7249 [ 0.2846; 1.1652]        5.7\nHazlett-Stevens & Oren 0.5287 [ 0.1162; 0.9412]        6.0\nHintz et al.           0.2840 [-0.0453; 0.6133]        6.9\nKang et al.            1.2751 [ 0.6142; 1.9360]        3.9\nKuhlmann et al.        0.1036 [-0.2781; 0.4853]        6.3\nLever Taylor et al.    0.3884 [-0.0639; 0.8407]        5.6\nPhang et al.           0.5407 [ 0.0619; 1.0196]        5.3\nRasanen et al.         0.4262 [-0.0794; 0.9317]        5.1\nRatanasiripong         0.5154 [-0.1731; 1.2039]        3.7\nShapiro et al.         1.4797 [ 0.8618; 2.0977]        4.2\nSong & Lindquist       0.6126 [ 0.1683; 1.0569]        5.7\nWarnecke et al.        0.6000 [ 0.1120; 1.0880]        5.2\n\nNumber of studies: k = 18\n\n                             SMD           95%-CI    t  p-value\nRandom effects model (HK) 0.5771 [0.3782; 0.7760] 6.12 &lt; 0.0001\n\nQuantifying heterogeneity (with 95%-CIs):\n tau^2 = 0.0820 [0.0295; 0.3533]; tau = 0.2863 [0.1717; 0.5944]\n I^2 = 62.6% [37.9%; 77.5%]; H = 1.64 [1.27; 2.11]\n\nTest of heterogeneity:\n     Q d.f. p-value\n 45.50   17  0.0002\n\nDetails of meta-analysis methods:\n- Inverse variance method\n- Restricted maximum-likelihood estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Calculation of I^2 based on Q\n- Hartung-Knapp adjustment for random effects model (df = 17)\n\n\n이분형효과에서의 합\n현재 library 에서는 binary outcome 에 대한 meta-analysis 는 metabin 함수를 이용합니다.\n\n\n2.2.2 metabin 함수 인수의 핵심 정리\nmetabin 함수에서 중요한 8가지 함수 인수는 다음과 같습니다.\n\nevent.e: 실험/치료 그룹에서 발생한 사건의 수.\nn.e: 실험/치료 그룹의 총 관측치 수.\nevent.c: 대조 그룹에서 발생한 사건의 수.\nn.c: 대조 그룹의 총 관측치 수.\nmethod: 풀링(pooling) 방법 지정:\n\n\"Inverse\": 일반적인 역분산 풀링.\n\"MH\": Mantel-Haenszel 방법 (고정 효과 모델에 권장, 기본값).\n\"Peto\": Peto 방법.\n\"SSW\": Bakbergenuly 표본 크기 방법 (sm = \"OR\" 일 때만 사용 가능).\n\nsm: 계산할 요약 측정값 (효과 크기 메트릭) 지정:\n\n\"RR\": 위험비 (Risk Ratio).\n\"OR\": 승산비 (Odds Ratio).\n\nincr: 영세포(zero cell) 연속성 보정 증분 값 지정:\n\nincr = 0.5: 0.5 증분 추가.\nincr = \"TACC\": 치료군 연속성 보정 방법 사용.\n일반적으로 생략 권장.\n\nMH.exact: method = \"MH\" 일 때, TRUE 설정 시 Mantel-Haenszel 방법 연속성 보정 미사용.\n\nDepressionMortality 데이터 세트는 Cuijpers와 Smit (2002)의 메타 분석을 기반으로 하는 DepressionMortality 데이터 세트를 예시로 사용합니다. 이 데이터 세트는 우울증이 전체 사망률에 미치는 영향을 조사합니다. 데이터 세트는 우울증이 있는 사람과 없는 사람의 수, 그리고 각 그룹에서 몇 년 후 사망한 사람의 수를 포함합니다.\n모델 유형\n\n고정효과 모델(Fixed-Effect Model):\n\n가정: 모든 연구가 동일한 진짜 효과 크기를 공유. 변동은 표본추출 오차(sampling error) 때문. - 적합 상황: 연구 간 이질성이 낮을 때.\n가중치: 역분산 가중치:\n\n\n\\[ w_i = \\frac{1}{SE_i^2} \\]\n\n랜덤효과 모델(Random-Effects Model):\n\n가정: 연구마다 진짜 효과 크기가 다를 수 있음. 표본추출 오차 외에 이질성 반영.\n적합 상황: 현실적으로 더 자주 사용.\n가중치: 이질성(τ²)을 포함한 수정 가중치:\n\n\n\\[ w_i = \\frac{1}{SE_i^2 + \\tau^2} \\]\n\nm.bin &lt;- metabin(event.e = event.e, \n                 n.e = n.e,\n                 event.c = event.c,\n                 n.c = n.c,\n                 studlab = author,\n                 data = DepressionMortality,\n                 sm = \"RR\",\n                 method = \"MH\",\n                 MH.exact = TRUE,\n                 fixed = TRUE,\n                 random = TRUE,\n                 method.tau = \"PM\",\n                 method.random.ci = \"HK\",\n                 title = \"Depression and Mortality\")\n\nWarning: Use argument 'common' instead of 'fixed' (deprecated).\n\nsummary(m.bin)\n\nReview:     Depression and Mortality\n\n                          RR            95%-CI %W(common) %W(random)\nAaroma et al., 1994   2.0998 [1.4128;  3.1208]        4.6        6.0\nBlack et al., 1998    1.7512 [1.3139;  2.3341]       11.6        6.6\nBruce et al., 1989    2.5183 [1.0785;  5.8802]        0.8        3.7\nBruce et al., 1994    1.1605 [0.8560;  1.5733]        9.0        6.5\nEnzell et al., 1984   1.8285 [1.2853;  2.6014]        6.8        6.3\nFredman et al., 1989  0.3971 [0.0566;  2.7861]        1.0        1.2\nMurphy et al., 1987   1.7640 [1.2644;  2.4610]        5.2        6.4\nPenninx et al., 1999  1.4647 [0.9361;  2.2919]        4.1        5.8\nPulska et al., 1998   1.9436 [1.3441;  2.8107]        3.1        6.2\nRoberts et al., 1990  2.3010 [1.9206;  2.7567]       23.7        7.0\nSaz et al., 1999      2.1837 [1.5533;  3.0700]        5.5        6.3\nSharma et al., 1998   2.0500 [1.0744;  3.9114]        2.5        4.7\nTakeida et al., 1997  6.9784 [4.1303; 11.7902]        1.5        5.3\nTakeida et al., 1999  5.8124 [3.8816;  8.7035]        3.3        6.0\nThomas et al., 1992   1.3303 [0.7780;  2.2745]        4.0        5.3\nThomas et al., 1992   1.7722 [1.1073;  2.8363]        4.1        5.6\nWeissman et al., 1986 1.2500 [0.6678;  2.3398]        2.7        4.8\nZheng et al., 1997    1.9803 [1.4001;  2.8011]        6.4        6.3\n\nNumber of studies: k = 18\nNumber of observations: o = 94770 (o.e = 4514, o.c = 90256)\nNumber of events: e = 5439\n\n                         RR           95%-CI   z|t  p-value\nCommon effect model  2.0634 [1.8909; 2.2516] 16.26 &lt; 0.0001\nRandom effects model 2.0217 [1.5786; 2.5892]  6.00 &lt; 0.0001\n\nQuantifying heterogeneity (with 95%-CIs):\n tau^2 = 0.1865 [0.0739; 0.5568]; tau = 0.4319 [0.2718; 0.7462]\n I^2 = 77.2% [64.3%; 85.4%]; H = 2.09 [1.67; 2.62]\n\nTest of heterogeneity:\n     Q d.f.  p-value\n 74.49   17 &lt; 0.0001\n\nDetails of meta-analysis methods:\n- Mantel-Haenszel method (common effect model)\n- Inverse variance method (random effects model)\n- Paule-Mandel estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Calculation of I^2 based on Q\n- Hartung-Knapp adjustment for random effects model (df = 17)\n\n\n\nsummary(m.bin) 결과 설명\n\n\n%W(common), %W(random): 각 연구가 고정 효과 모델과 랜덤 효과 모델에서 전체 결과에 미치는 가중치를 백분율로 나타냅니다.\nNumber of studies: k = 18: 분석에 포함된 연구의 수입니다.\nNumber of observations: o = 94770 (o.e = 4514, o.c = 90256): 총 관측치 수와 실험군, 대조군의 관측치 수입니다.\nNumber of events: e = 5439: 총 사건 발생 수입니다.\nCommon effect model, Random effects model: 고정 효과 모델과 랜덤 효과 모델의 종합 위험비와 95% 신뢰구간, z/t 값, p 값을 나타냅니다.\n\nRR 2.0634 [1.8909; 2.2516]: 고정 효과 모델에서의 종합 위험비는 2.06이며, 95% 신뢰구간은 1.89에서 2.25입니다. 이는 우울증 그룹의 사망 위험이 비우울증 그룹보다 약 2.06배 높다는 것을 의미합니다.\nRR 2.0217 [1.5786; 2.5892]: 랜덤 효과 모델에서의 종합 위험비는 2.02이며, 95% 신뢰구간은 1.58에서 2.59입니다.\nz|t, p-value: 통계적 유의성을 나타냅니다. p 값이 0.0001보다 작으므로 통계적으로 유의미한 결과를 보여줍니다.\n\nQuantifying heterogeneity: 연구 간 이질성을 나타냅니다.\n\ntau^2, tau: 연구 간 분산과 표준편차를 나타냅니다.\nI^2: 이질성의 정도를 백분율로 나타냅니다. 77.2%는 높은 이질성을 의미합니다.\n\nI² (I-squared): 이질성 정도\n\n의미: 총 관측된 이질성 중 실제 이질성이 차지하는 비율을 백분율로 나타냅니다.\n해석:\n\n0~25%: 낮은 이질성\n25~50%: 중간 정도의 이질성\n50~75%: 높은 이질성\n75% 이상: 매우 높은 이질성 주어진 결과: I² = 77.2%이므로, 연구들 간에 매우 높은 이질성이 존재함을 나타냅니다.\n\n\n\nH: 이질성의 정도를 나타내는 또 다른 지표입니다.\n\nTest of heterogeneity: 이질성의 통계적 유의성을 검정합니다.\n\nQ, d.f., p-value: 이질성 검정의 통계량, 자유도, p 값을 나타냅니다. p 값이 0.0001보다 작으므로 이질성이 통계적으로 유의미합니다.\n\n\n\n\n2.2.3 사전 계산된 효과 크기 데이터의 필요성:\n일부 연구에서는 위험비(RR) 또는 승산비(OR)를 계산하는 데 필요한 원시 데이터를 제공하지 않을 수 있습니다. 이 경우, 이미 계산된 효과 크기 데이터(예: RR, OR, 신뢰구간)를 사용하여 메타 분석을 수행해야 합니다.\n\nDepressionMortality$TE &lt;- m.bin$TE\nDepressionMortality$seTE &lt;- m.bin$seTE\n\n\n# Set seTE of study 7 to NA\nDepressionMortality$seTE[7] &lt;- NA\n\n# Create empty columns 'lower' and 'upper'\nDepressionMortality[,\"lower\"] &lt;- NA\nDepressionMortality[,\"upper\"] &lt;- NA\n\n# Fill in values for 'lower' and 'upper' in study 7\n# As always, binary effect sizes need to be log-transformed\nDepressionMortality$lower[7] &lt;- log(1.26)\nDepressionMortality$upper[7] &lt;- log(2.46)\n\n\nDepressionMortality[,c(\"author\", \"TE\", \"seTE\", \"lower\", \"upper\")]\n\n                  author         TE       seTE     lower     upper\n1    Aaroma et al., 1994  0.7418532 0.20217061        NA        NA\n2     Black et al., 1998  0.5603039 0.14659887        NA        NA\n3     Bruce et al., 1989  0.9235782 0.43266994        NA        NA\n4     Bruce et al., 1994  0.1488720 0.15526121        NA        NA\n5    Enzell et al., 1984  0.6035076 0.17986279        NA        NA\n6   Fredman et al., 1989 -0.9236321 0.99403676        NA        NA\n7    Murphy et al., 1987  0.5675840         NA 0.2311117 0.9001613\n8   Penninx et al., 1999  0.3816630 0.22842369        NA        NA\n9    Pulska et al., 1998  0.6645639 0.18819368        NA        NA\n10  Roberts et al., 1990  0.8333374 0.09218909        NA        NA\n11      Saz et al., 1999  0.7810180 0.17380951        NA        NA\n12   Sharma et al., 1998  0.7178398 0.32962024        NA        NA\n13  Takeida et al., 1997  1.9428138 0.26758609        NA        NA\n14  Takeida et al., 1999  1.7599912 0.20599112        NA        NA\n15   Thomas et al., 1992  0.2853747 0.27366749        NA        NA\n16   Thomas et al., 1992  0.5721946 0.23995571        NA        NA\n17 Weissman et al., 1986  0.2231436 0.31986687        NA        NA\n18    Zheng et al., 1997  0.6832705 0.17690650        NA        NA\n\n\nmetagen 함수:\n\n사전 계산된 효과 크기 데이터를 사용하여 메타 분석을 수행하는 데 사용됩니다.\nmetabin 함수와 달리, metagen 함수는 효과 크기를 풀링할 때 항상 역분산 방법(inverse-variance method)을 사용합니다.\nMantel-Haenszel 방법과 같은 다른 방법은 사용할 수 없습니다.\n\n데이터 준비:\n\nDepressionMortality 데이터 세트를 사용하여 사전 계산된 효과 크기 메타 분석을 시뮬레이션합니다.\nm.bin 객체에서 각 연구의 효과 크기(TE)와 표준 오차(seTE)를 추출하여 DepressionMortality 데이터 세트에 저장합니다.\n일부 연구(예: Murphy et al., 1987)의 경우 표준 오차를 사용할 수 없고 신뢰구간의 하한(lower) 및 상한(upper)만 사용할 수 있는 상황을 시뮬레이션합니다.\n\nmetagen 함수 사용:\n\nmetagen 함수를 사용하여 사전 계산된 효과 크기 데이터를 풀링합니다.\nTE, seTE, lower, upper 인수를 사용하여 효과 크기, 표준 오차, 신뢰구간 정보를 제공합니다.\nmethod.tau = “PM” 인수를 사용하여 연구 간 이질성(tau²)을 Paule-Mandel 방법으로 추정합니다.\nfixed = FALSE, random = TRUE 인수를 사용하여 랜덤 효과 모델을 사용합니다.\n\n\nm.gen_bin &lt;- metagen(TE = TE,\n                     seTE = seTE,\n                     lower = lower,\n                     upper = upper,\n                     studlab = author,\n                     data = DepressionMortality,\n                     sm = \"RR\",\n                     method.tau = \"PM\",\n                     fixed = FALSE,\n                     random = TRUE,\n                     title = \"Depression Mortality (Pre-calculated)\")\n\nWarning: Use argument 'common' instead of 'fixed' (deprecated).\n\nsummary(m.gen_bin)\n\nReview:     Depression Mortality (Pre-calculated)\n\n                          RR            95%-CI %W(random)\nAaroma et al., 1994   2.0998 [1.4128;  3.1208]        6.0\nBlack et al., 1998    1.7512 [1.3139;  2.3341]        6.6\nBruce et al., 1989    2.5183 [1.0785;  5.8802]        3.7\nBruce et al., 1994    1.1605 [0.8560;  1.5733]        6.5\nEnzell et al., 1984   1.8285 [1.2853;  2.6014]        6.3\nFredman et al., 1989  0.3971 [0.0566;  2.7861]        1.2\nMurphy et al., 1987   1.7640 [1.2600;  2.4600]        6.4\nPenninx et al., 1999  1.4647 [0.9361;  2.2919]        5.8\nPulska et al., 1998   1.9436 [1.3441;  2.8107]        6.2\nRoberts et al., 1990  2.3010 [1.9206;  2.7567]        7.1\nSaz et al., 1999      2.1837 [1.5533;  3.0700]        6.3\nSharma et al., 1998   2.0500 [1.0744;  3.9114]        4.7\nTakeida et al., 1997  6.9784 [4.1303; 11.7902]        5.3\nTakeida et al., 1999  5.8124 [3.8816;  8.7035]        6.0\nThomas et al., 1992   1.3303 [0.7780;  2.2745]        5.3\nThomas et al., 1992   1.7722 [1.1073;  2.8363]        5.6\nWeissman et al., 1986 1.2500 [0.6678;  2.3398]        4.8\nZheng et al., 1997    1.9803 [1.4001;  2.8011]        6.3\n\nNumber of studies: k = 18\n\n                         RR           95%-CI    z  p-value\nRandom effects model 2.0218 [1.6066; 2.5442] 6.00 &lt; 0.0001\n\nQuantifying heterogeneity (with 95%-CIs):\n tau^2 = 0.1865 [0.0739; 0.5568]; tau = 0.4319 [0.2718; 0.7462]\n I^2 = 77.2% [64.3%; 85.4%]; H = 2.09 [1.67; 2.62]\n\nTest of heterogeneity:\n     Q d.f.  p-value\n 74.49   17 &lt; 0.0001\n\nDetails of meta-analysis methods:\n- Inverse variance method\n- Paule-Mandel estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Calculation of I^2 based on Q\n\n\n결과 해석:\n\nsummary(m.gen_bin) 함수를 사용하여 메타 분석 결과를 요약합니다.\n결과는 metabin 함수를 사용하여 원시 데이터를 풀링한 결과와 거의 동일합니다.\nmetagen 함수는 표준 오차가 없는 연구의 경우 신뢰구간 정보를 사용하여 효과 크기를 풀링할 수 있음을 보여줍니다.\n이것은 실제 메타분석에서 흔히 발생하는 상황에 효과적으로 대응할 수 있게 해줍니다.\n\n\n\n2.2.4 Forest Plots\n포레스트 플롯(forest plot)은 메타 분석 결과를 시각적으로 요약하여 보여주는 그래프입니다. 여러 연구의 효과 크기(effect size)와 신뢰 구간(confidence interval)을 한눈에 비교하고 전체 효과 크기를 파악하는 데 유용합니다.\n\nmeta::forest(m.gen, \n             sortvar = TE,\n             prediction = TRUE, \n             print.tau2 = FALSE,\n             leftlabs = c(\"Author\", \"g\", \"SE\")) #Hedges'g\n\n\n\n\n\n\n\n\n\nmeta::forest(m.gen, layout = \"JAMA\")\n\n\n\n\n\n\n\n\n\nforest(m.bin,\n       leftcols = c(\"studlab\", \"event.e\", \"n.e\", \"event.c\", \"n.c\"),\n       rightcols = c(\"effect\", \"ci\"),\n       xlab = \"Risk Ratio (RR)\",\n       main = \"Forest Plot of Depression and Mortality\",\n       digits = 2)\n\n\n\n\n\n\n\n\n\nlibrary(meta)\n\nforest(m.gen_bin,\n       leftcols = c(\"studlab\", \"TE\", \"seTE\", \"lower\", \"upper\"),\n       rightcols = c(\"effect\", \"ci\"),\n       xlab = \"Risk Ratio (RR)\",\n       main = \"Forest Plot of Depression Mortality (Pre-calculated)\",\n       digits = 2) \n\n\n\n\n\n\n\n       #layout =\"RevMan5\")\n\n\n\n2.2.5 하위(층하) 분석\n아래 데이터에는 RiskOfBias라는 집단 데이터가 있습니다. 각 집단에 대해서 층화/하위 그룹 분석을 수행해봅니다.\n\nm.gen$data %&gt;% head()\n\n           Author        TE      seTE RiskOfBias TypeControlGroup\n1     Call et al. 0.7091362 0.2608202       high              WLC\n2 Cavanagh et al. 0.3548641 0.1963624        low              WLC\n3   DanitzOrsillo 1.7911700 0.3455692       high              WLC\n4  de Vibe et al. 0.1824552 0.1177874        low  no intervention\n5  Frazier et al. 0.4218509 0.1448128        low information only\n6  Frogeli et al. 0.6300000 0.1960000        low  no intervention\n  InterventionDuration InterventionType ModeOfDelivery        .studlab\n1                short      mindfulness          group     Call et al.\n2                short      mindfulness         online Cavanagh et al.\n3                short              ACT          group   DanitzOrsillo\n4                short      mindfulness          group  de Vibe et al.\n5                short              PCI         online  Frazier et al.\n6                short              ACT          group  Frogeli et al.\n        .TE     .seTE\n1 0.7091362 0.2608202\n2 0.3548641 0.1963624\n3 1.7911700 0.3455692\n4 0.1824552 0.1177874\n5 0.4218509 0.1448128\n6 0.6300000 0.1960000\n\n\n\nupdate(m.gen, subgroup = RiskOfBias, tau.common = TRUE)\n\nReview:     Third Wave Psychotherapies\n\nNumber of studies: k = 18\n\n                             SMD           95%-CI    t  p-value\nRandom effects model (HK) 0.5771 [0.3782; 0.7760] 6.12 &lt; 0.0001\n\nQuantifying heterogeneity (with 95%-CIs):\n tau^2 = 0.0820 [0.0295; 0.3533]; tau = 0.2863 [0.1717; 0.5944]\n I^2 = 62.6% [37.9%; 77.5%]; H = 1.64 [1.27; 2.11]\n\nQuantifying residual heterogeneity (with 95%-CIs):\n tau^2 = 0.0691 [0.0208; 0.3268]; tau = 0.2630 [0.1441; 0.5717]\n I^2 = 59.3% [30.6%; 76.1%]; H = 1.57 [1.20; 2.05]\n\nTest of heterogeneity:\n     Q d.f. p-value\n 45.50   17  0.0002\n\nResults for subgroups (random effects model (HK)):\n                    k    SMD           95%-CI  tau^2    tau     Q   I^2\nRiskOfBias = high   7 0.7691 [0.2533; 1.2848] 0.0691 0.2630 25.89 76.8%\nRiskOfBias = low   11 0.4698 [0.3015; 0.6382] 0.0691 0.2630 13.42 25.5%\n\nTest for subgroup differences (random effects model (HK)):\n                   Q d.f. p-value\nBetween groups  1.79    1  0.1814\nWithin groups  39.31   16  0.0010\n\nDetails of meta-analysis methods:\n- Inverse variance method\n- Restricted maximum-likelihood estimator for tau^2\n  (assuming common tau^2 in subgroups)\n- Q-Profile method for confidence interval of tau^2 and tau\n- Calculation of I^2 based on Q\n- Hartung-Knapp adjustment for random effects model (df = 17)\n\n\nSubgroup 분석:\n\n위험 편향 (RiskOfBias) 기준: 연구들을 “high”와 “low” 위험 편향 그룹으로 나누어 분석했습니다.\n\nhigh 위험 편향 그룹 (7개 연구): SMD = 0.7691 (95% CI: 0.2533, 1.2848), I² = 76.8%로, 높은 이질성을 보입니다.\nlow 위험 편향 그룹 (11개 연구): SMD = 0.4698 (95% CI: 0.3015, 0.6382), I² = 25.5%로, 상대적으로 낮은 이질성을 보입니다.\n\n그룹 간 차이 검정:\n\n그룹 간 차이 검정 결과 (Q = 1.79, p = 0.1814)는 그룹 간 효과 크기 차이가 통계적으로 유의하지 않음을 나타냅니다.\n그룹 내 이질성 검정 결과 (Q = 39.31, p = 0.0010)는 그룹 내에서 여전히 상당한 이질성이 존재함을 나타냅니다.\n\n\n\n\n2.2.6 메타 회귀 분석석\n\nyear &lt;- c(2014, 1998, 2010, 1999, 2005, 2014, \n          2019, 2010, 1982, 2020, 1978, 2001,\n          2018, 2002, 2009, 2011, 2011, 2013)\nm.gen.reg &lt;- metareg(m.gen, ~year)\nm.gen.reg\n\n\nMixed-Effects Model (k = 18; tau^2 estimator: REML)\n\ntau^2 (estimated amount of residual heterogeneity):     0.0188 (SE = 0.0226)\ntau (square root of estimated tau^2 value):             0.1371\nI^2 (residual heterogeneity / unaccounted variability): 29.26%\nH^2 (unaccounted variability / sampling variability):   1.41\nR^2 (amount of heterogeneity accounted for):            77.08%\n\nTest for Residual Heterogeneity:\nQE(df = 16) = 27.8273, p-val = 0.0332\n\nTest of Moderators (coefficient 2):\nF(df1 = 1, df2 = 16) = 9.3755, p-val = 0.0075\n\nModel Results:\n\n         estimate       se     tval  df    pval     ci.lb     ci.ub     \nintrcpt  -36.1546  11.9800  -3.0179  16  0.0082  -61.5510  -10.7582  ** \nyear       0.0183   0.0060   3.0619  16  0.0075    0.0056    0.0310  ** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n층화 분석 결과:\n\n전체 이질성 (I²): 62.6%\n잔차 이질성 (I²): 59.3%\n\n메타 회귀 분석 결과:\n\n잔차 이질성 (I²): 29.26%\nR² (설명된 이질성): 77.08%\n\n비교 및 해석:\n\n이질성 감소: 메타 회귀 분석 결과, 잔차 이질성(I²)이 29.26%로 크게 감소했습니다. 이는 연도(year)가 연구 간의 이질성을 상당 부분 설명한다는 것을 의미합니다.\n설명된 이질성: R² 값이 77.08%로, 연도가 이질성의 약 77%를 설명하는 것으로 나타났습니다.\n유의성:\n\n잔차 이질성 검정 결과 (QE p-value = 0.0332)는 잔차 이질성이 여전히 존재하지만, 층화 분석에 비해 크게 감소했음을 시사합니다.\n예측변수(연도)의 유의성 검정 결과 (F p-value = 0.0075)는 연도가 효과 크기에 유의한 영향을 미친다는 것을 나타냅니다.\n\n\n\nlibrary(meta)\nlibrary(stringr)\n\n# 년도 추출 (m.bin$data$author 가정)\nyears &lt;- as.numeric(str_extract(m.bin$data$author, \"\\\\d{4}\"))\n\n# metareg 함수를 사용하여 메타 회귀 분석 수행\nm.bin$data$year &lt;- years # m.bin$data에 years 변수 추가\nmetareg_model &lt;- metareg(m.bin, ~ year)\n\nWarning in metareg.meta(m.bin, ~year): R object 'year' found in .GlobalEnv used\nin meta-regression instead of data from meta-analysis object.\n\n# 결과 요약\nsummary(metareg_model)\n\n\nMixed-Effects Model (k = 18; tau^2 estimator: PM)\n\n  logLik  deviance       AIC       BIC      AICc   \n-13.0154   46.1428   32.0308   34.7019   33.7451   \n\ntau^2 (estimated amount of residual heterogeneity):     0.2022 (SE = 0.0934)\ntau (square root of estimated tau^2 value):             0.4497\nI^2 (residual heterogeneity / unaccounted variability): 83.46%\nH^2 (unaccounted variability / sampling variability):   6.04\nR^2 (amount of heterogeneity accounted for):            0.00%\n\nTest for Residual Heterogeneity:\nQE(df = 16) = 73.0318, p-val &lt; .0001\n\nTest of Moderators (coefficient 2):\nF(df1 = 1, df2 = 16) = 0.0773, p-val = 0.7846\n\nModel Results:\n\n         estimate       se     tval  df    pval     ci.lb    ci.ub    \nintrcpt   -5.0251  20.6057  -0.2439  16  0.8104  -48.7071  38.6570    \nyear       0.0029   0.0103   0.2780  16  0.7846   -0.0189   0.0246    \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# 버블 플롯 생성\nbubble(metareg_model, studlab = TRUE)\n\n\n\n\n\n\n\n\n\nlibrary(meta) 및 library(stringr): meta 패키지 (메타 회귀 분석용)와 stringr 패키지 (문자열 처리용)를 로드합니다.\n년도 추출:\n\nyears &lt;- as.numeric(str_extract(m.bin\\(data\\)author, “\\d{4}”)): 이 줄은 m.bin\\(data\\)author에서 년도를 추출하는 데 사용됩니다. str_extract 함수는 문자열에서 정규 표현식과 일치하는 첫 번째 하위 문자열을 찾습니다. \\d{4}는 정확히 4개의 숫자와 일치하는 정규 표현식입니다. 추출된 년도는 as.numeric()을 사용하여 숫자 형식으로 변환됩니다.\n\nmetareg 함수를 사용하여 메타 회귀 분석 수행:\n\nm.bin\\(data\\)year &lt;- years: 추출한 years 변수를 m.bin$data 데이터프레임에 year라는 이름으로 추가합니다. metareg 함수는 메타 분석 객체와 함께 독립 변수의 이름을 지정하는 방식으로 사용되므로, 변수를 데이터프레임에 포함시켜야 합니다.\nmetareg_model &lt;- metareg(m.bin, ~ year): metareg 함수를 사용하여 메타 회귀 분석을 수행합니다. 첫 번째 인수로 m.bin 객체를 전달하고, 두 번째 인수로 회귀 모델을 지정합니다. ~ year는 year 변수를 독립 변수로 사용하는 모델을 나타냅니다.\n\n결과 요약:\n\nsummary(metareg_model): 이 줄은 metareg_model 객체에 저장된 메타 회귀 분석 결과를 요약하여 출력합니다. 이 요약에는 추정된 회귀 계수, 통계량 및 p 값이 포함됩니다.\n\n버블 플롯 생성:\n\nbubble(metareg_model, studlab = TRUE): 이 줄은 메타 회귀 분석 결과를 시각화하는 버블 플롯을 생성합니다.\n\nmetareg_model: 이전 단계에서 생성된 metareg 객체입니다.\nstudlab = TRUE: 연구 라벨을 표시합니다.\n\n\n\n\n\n2.2.7 출판오류\n\n# Define fill colors for contour\ncol.contour = c(\"gray75\", \"gray85\", \"gray95\")\n\n# Generate funnel plot (we do not include study labels here)\nmeta::funnel(m.gen, xlim = c(-0.5, 2),\n             contour = c(0.9, 0.95, 0.99),\n             col.contour = col.contour)\n\n# Add a legend\nlegend(x = 1.6, y = 0.01, \n       legend = c(\"p &lt; 0.1\", \"p &lt; 0.05\", \"p &lt; 0.01\"),\n       fill = col.contour)\n\n# Add a title\ntitle(\"Contour-Enhanced Funnel Plot (Third Wave Psychotherapies)\")\n\n\n\n\n\n\n\n\n이 글은 윤곽선 강화 깔때기 그림(Contour-Enhanced Funnel Plot)을 통해 연구 결과의 출판 편향 가능성을 분석한 내용을 담고 있습니다. 깔때기 그림은 연구들의 효과 크기와 표준 오차를 시각적으로 보여주며, 특히 p &lt; 0.05 및 p &lt; 0.01 영역에 속하는 연구들은 전통적으로 통계적 유의성을 인정받습니다.\n윤곽선을 추가하여 분석한 결과, 표준 오차가 큰 작은 연구들은 유의한 효과를 보이지만, 비슷한 표준 오차를 가진 다른 연구는 유의하지 않은 결과를 보였습니다. 또한, 그림의 왼쪽 아래 모서리에는 연구가 부족하여 비대칭적인 패턴을 보였으며, 이는 누락된 연구들이 유의하지 않거나 음의 효과를 보일 수 있음을 시사합니다.\n표준 오차가 작은 큰 연구들의 경우, 대부분 유의하지 않지만 유의성 경계에 근접한 결과를 보였습니다. 이는 연구자들이 효과 크기를 다르게 계산했거나, 경향적 유의성만으로도 출판이 가능했을 가능성을 시사합니다.\n\n# Using all studies\ntf &lt;- trimfill(m.gen)\n\n# Analyze with outliers removed\ntf.no.out &lt;- trimfill(update(m.gen, \n                             subset = -c(3, 16)))\n# Define fill colors for contour\ncontour &lt;- c(0.9, 0.95, 0.99)\ncol.contour &lt;- c(\"gray75\", \"gray85\", \"gray95\")\nld &lt;- c(\"p &lt; 0.1\", \"p &lt; 0.05\", \"p &lt; 0.01\")\n\n# Use 'par' to create two plots in one row (row, columns)\npar(mfrow=c(1,2))\n\n# Contour-enhanced funnel plot (full data)\nmeta::funnel(tf, \n             xlim = c(-1.5, 2), contour = contour,\n             col.contour = col.contour)\nlegend(x = 1.1, y = 0.01, \n       legend = ld, fill = col.contour)\ntitle(\"Funnel Plot (Trim & Fill Method)\")\n\n# Contour-enhanced funnel plot (outliers removed)\nmeta::funnel(tf.no.out, \n            xlim = c(-1.5, 2), contour = contour,\n            col.contour = col.contour)\nlegend(x = 1.1, y = 0.01, \n       legend = ld, fill = col.contour)\ntitle(\"Funnel Plot (Trim & Fill Method) - Outliers Removed\")\n\n\n\n\n\n\n\n\n\nlibrary(meta)\nlibrary(stringr)\n\n# 년도 추출 (m.bin$data$author 가정)\nyears &lt;- as.numeric(str_extract(m.bin$data$author, \"\\\\d{4}\"))\nm.bin$data$year &lt;- years\nRR &lt;- exp(m.bin$TE)\n\n# 윤곽선 색상 정의\ncol.contour &lt;- c(\"gray75\", \"gray85\", \"gray95\")\n\n# 깔때기 그림 생성 (yaxis 제거)\nmeta::funnel(m.bin,\n            xlim = c(min(RR) - 0.1, max(RR) + 5), # TE 범위에 맞춰 xlim 조정\n            contour = c(0.9, 0.95, 0.99),\n            col.contour = col.contour)\n\n# 범례 추가\nlegend(x = max(RR), y = 0.01,\n       legend = c(\"p &lt; 0.1\", \"p &lt; 0.05\", \"p &lt; 0.01\"),\n       fill = col.contour)\n\n# 제목 추가\ntitle(\"Contour-Enhanced Funnel Plot (Depression and Mortality)\")\n\n\n\n\n\n\n\n# trim and fill 적용\ntrimfill_model &lt;- trimfill(m.bin)\n\n# trim and fill 결과 출력\nsummary(trimfill_model)\n\nReview:     Depression and Mortality\n\n                                   RR            95%-CI %W(random)\nAaroma et al., 1994            2.0998 [1.4128;  3.1208]        4.9\nBlack et al., 1998             1.7512 [1.3139;  2.3341]        5.2\nBruce et al., 1989             2.5183 [1.0785;  5.8802]        3.2\nBruce et al., 1994             1.1605 [0.8560;  1.5733]        5.2\nEnzell et al., 1984            1.8285 [1.2853;  2.6014]        5.0\nFredman et al., 1989           0.3971 [0.0566;  2.7861]        1.1\nMurphy et al., 1987            1.7640 [1.2644;  2.4610]        5.1\nPenninx et al., 1999           1.4647 [0.9361;  2.2919]        4.7\nPulska et al., 1998            1.9436 [1.3441;  2.8107]        5.0\nRoberts et al., 1990           2.3010 [1.9206;  2.7567]        5.5\nSaz et al., 1999               2.1837 [1.5533;  3.0700]        5.1\nSharma et al., 1998            2.0500 [1.0744;  3.9114]        3.9\nTakeida et al., 1997           6.9784 [4.1303; 11.7902]        4.4\nTakeida et al., 1999           5.8124 [3.8816;  8.7035]        4.8\nThomas et al., 1992            1.3303 [0.7780;  2.2745]        4.3\nThomas et al., 1992            1.7722 [1.1073;  2.8363]        4.6\nWeissman et al., 1986          1.2500 [0.6678;  2.3398]        4.0\nZheng et al., 1997             1.9803 [1.4001;  2.8011]        5.0\nFilled: Penninx et al., 1999   3.4400 [2.1985;  5.3826]        4.7\nFilled: Thomas et al., 1992    3.7877 [2.2153;  6.4762]        4.3\nFilled: Weissman et al., 1986  4.0309 [2.1534;  7.5452]        4.0\nFilled: Bruce et al., 1994     4.3417 [3.2026;  5.8859]        5.2\nFilled: Fredman et al., 1989  12.6893 [1.8085; 89.0358]        1.1\n\nNumber of studies: k = 23 (with 5 added studies)\nNumber of observations: o = 104399 (o.e = 4945, o.c = 99454)\nNumber of events: e = 7307\n\n                         RR           95%-CI    t  p-value\nRandom effects model 2.3189 [1.8272; 2.9428] 7.32 &lt; 0.0001\n\nQuantifying heterogeneity (with 95%-CIs):\n tau^2 = 0.2308 [0.1021; 0.6192]; tau = 0.4804 [0.3195; 0.7869]\n I^2 = 80.2% [71.0%; 86.5%]; H = 2.25 [1.86; 2.72]\n\nTest of heterogeneity:\n      Q d.f.  p-value\n 111.06   22 &lt; 0.0001\n\nDetails of meta-analysis methods:\n- Inverse variance method\n- Paule-Mandel estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Calculation of I^2 based on Q\n- Hartung-Knapp adjustment for random effects model (df = 22)\n- Trim-and-fill method to adjust for funnel plot asymmetry (L-estimator)\n\n# trim and fill 적용 후 깔때기 그림\nmeta::funnel(trimfill_model,\n            xlim = c(min(RR) - 0.1, max(RR) + 5),\n            contour = c(0.9, 0.95, 0.99),\n            col.contour = col.contour)\n\n# 범례 추가\nlegend(x = max(RR), y = 0.01,\n       legend = c(\"p &lt; 0.1\", \"p &lt; 0.05\", \"p &lt; 0.01\"),\n       fill = col.contour)\n\n# 제목 추가\ntitle(\"Trim and Fill Adjusted Funnel Plot (Depression and Mortality)\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>메타분석</span>"
    ]
  },
  {
    "objectID": "MetaAnalysis.html#효과-크기-통합-pooling-effect-sizes",
    "href": "MetaAnalysis.html#효과-크기-통합-pooling-effect-sizes",
    "title": "2  메타분석",
    "section": "2.3 효과 크기 통합 (Pooling Effect Sizes)",
    "text": "2.3 효과 크기 통합 (Pooling Effect Sizes)\n통합의 필요성\n\n효과 크기 통합 은 개별 연구의 결과를 하나로 합쳐 전체적인 효과를 추정합니다.\n이를 통해 연구 간 일관성 여부와 평균 효과를 확인할 수 있습니다.\n결국 평균을 내는데, 각 연구의 가중치를 부여한다. 입니다.\n\nSuicidePrevention은 dmetar 패키지에 포함된 예제 데이터셋으로, 자살 예방 프로그램의 효과를 평가한 연구 데이터를 담고 있습니다. 이 데이터는 보통 다음과 같은 열(column)을 포함합니다:\n\nmean.e: 실험군(experimental group)의 평균\nsd.e: 실험군의 표준편차\nn.e: 실험군의 표본 크기\nmean.c: 대조군(control group)의 평균\nsd.c: 대조군의 표준편차\nn.c: 대조군의 표본 크기\nauthor: 연구 저자 또는 연구 이름\n\n\nlibrary(dmetar)\nlibrary(esc)\nlibrary(tidyverse)\n# Load data set from dmetar\ndata(SuicidePrevention)\n\n# Calculate Hedges' g and the Standard Error\n# - We save the study names in \"study\".\n# - We use the pmap_dfr function to calculate the effect size\n#   for each row.\nSP_calc &lt;- pmap_dfr(SuicidePrevention, \n                    function(mean.e, sd.e, n.e, mean.c,\n                             sd.c, n.c, author, ...){\n                      esc_mean_sd(grp1m = mean.e,\n                                  grp1sd = sd.e,\n                                  grp1n = n.e,\n                                  grp2m = mean.c,\n                                  grp2sd = sd.c,\n                                  grp2n = n.c,\n                                  study = author,\n                                  es.type = \"g\") %&gt;%  #d : cohen's d\n                        as.data.frame()}) \n\n# Let us catch a glimpse of the data\n# The data set contains Hedges' g (\"es\") and standard error (\"se\")\nSP_calc %&gt;% head()\n\n            study         es   weight sample.size        se        var\n1    Berry et al. -0.1427945 46.09784         185 0.1472854 0.02169299\n2  DeVries et al. -0.6077093 34.77314         146 0.1695813 0.02875783\n3  Fleming et al. -0.1111797 14.97625          60 0.2584036 0.06677240\n4    Hunt & Burke -0.1269801 32.18243         129 0.1762749 0.03107286\n5 McCarthy et al. -0.3924744 24.52054         100 0.2019459 0.04078214\n6   Meijer et al. -0.2675788 54.50431         220 0.1354517 0.01834717\n       ci.lo        ci.hi measure\n1 -0.4314686  0.145879624       g\n2 -0.9400826 -0.275335960       g\n3 -0.6176413  0.395282029       g\n4 -0.4724727  0.218512440       g\n5 -0.7882811  0.003332195       g\n6 -0.5330593 -0.002098274       g\n\n\n\nSP_calc$w &lt;- 1/SP_calc$se^2\n\n# Then, we use the weights to calculate the pooled effect\npooled_effect &lt;- sum(SP_calc$w*SP_calc$es)/sum(SP_calc$w)\npooled_effect\n\n[1] -0.2311121\n\n\n이분형효과에서의 합\n현재 library 에서는 binary outcome 에 대한 meta-analysis 는 metabin 함수를 이용합니다.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>메타분석</span>"
    ]
  },
  {
    "objectID": "MetaAnalysis.html#metabin-함수-인수의-핵심-정리",
    "href": "MetaAnalysis.html#metabin-함수-인수의-핵심-정리",
    "title": "2  메타분석",
    "section": "2.4 metabin 함수 인수의 핵심 정리",
    "text": "2.4 metabin 함수 인수의 핵심 정리\nmetabin 함수에서 중요한 8가지 함수 인수는 다음과 같습니다.\n\nevent.e: 실험/치료 그룹에서 발생한 사건의 수.\nn.e: 실험/치료 그룹의 총 관측치 수.\nevent.c: 대조 그룹에서 발생한 사건의 수.\nn.c: 대조 그룹의 총 관측치 수.\nmethod: 풀링(pooling) 방법 지정:\n\n\"Inverse\": 일반적인 역분산 풀링.\n\"MH\": Mantel-Haenszel 방법 (고정 효과 모델에 권장, 기본값).\n\"Peto\": Peto 방법.\n\"SSW\": Bakbergenuly 표본 크기 방법 (sm = \"OR\" 일 때만 사용 가능).\n\nsm: 계산할 요약 측정값 (효과 크기 메트릭) 지정:\n\n\"RR\": 위험비 (Risk Ratio).\n\"OR\": 승산비 (Odds Ratio).\n\nincr: 영세포(zero cell) 연속성 보정 증분 값 지정:\n\nincr = 0.5: 0.5 증분 추가.\nincr = \"TACC\": 치료군 연속성 보정 방법 사용.\n일반적으로 생략 권장.\n\nMH.exact: method = \"MH\" 일 때, TRUE 설정 시 Mantel-Haenszel 방법 연속성 보정 미사용.\n\nDepressionMortality 데이터 세트는 Cuijpers와 Smit (2002)의 메타 분석을 기반으로 하는 DepressionMortality 데이터 세트를 예시로 사용합니다. 이 데이터 세트는 우울증이 전체 사망률에 미치는 영향을 조사합니다. 데이터 세트는 우울증이 있는 사람과 없는 사람의 수, 그리고 각 그룹에서 몇 년 후 사망한 사람의 수를 포함합니다.\n모델 유형\n\n고정효과 모델(Fixed-Effect Model):\n\n가정: 모든 연구가 동일한 진짜 효과 크기를 공유. 변동은 표본추출 오차(sampling error) 때문. - 적합 상황: 연구 간 이질성이 낮을 때.\n가중치: 역분산 가중치:\n\n\n\\[ w_i = \\frac{1}{SE_i^2} \\]\n\n랜덤효과 모델(Random-Effects Model):\n\n가정: 연구마다 진짜 효과 크기가 다를 수 있음. 표본추출 오차 외에 이질성 반영.\n적합 상황: 현실적으로 더 자주 사용.\n가중치: 이질성(τ²)을 포함한 수정 가중치:\n\n\n\\[ w_i = \\frac{1}{SE_i^2 + \\tau^2} \\]\n\nm.bin &lt;- metabin(event.e = event.e, \n                 n.e = n.e,\n                 event.c = event.c,\n                 n.c = n.c,\n                 studlab = author,\n                 data = DepressionMortality,\n                 sm = \"RR\",\n                 method = \"MH\",\n                 MH.exact = TRUE,\n                 fixed = TRUE,\n                 random = TRUE,\n                 method.tau = \"PM\",\n                 method.random.ci = \"HK\",\n                 title = \"Depression and Mortality\")\n\nWarning: Use argument 'common' instead of 'fixed' (deprecated).\n\nsummary(m.bin)\n\nReview:     Depression and Mortality\n\n                          RR            95%-CI %W(common) %W(random)\nAaroma et al., 1994   2.0998 [1.4128;  3.1208]        4.6        6.0\nBlack et al., 1998    1.7512 [1.3139;  2.3341]       11.6        6.6\nBruce et al., 1989    2.5183 [1.0785;  5.8802]        0.8        3.7\nBruce et al., 1994    1.1605 [0.8560;  1.5733]        9.0        6.5\nEnzell et al., 1984   1.8285 [1.2853;  2.6014]        6.8        6.3\nFredman et al., 1989  0.3971 [0.0566;  2.7861]        1.0        1.2\nMurphy et al., 1987   1.7640 [1.2644;  2.4610]        5.2        6.4\nPenninx et al., 1999  1.4647 [0.9361;  2.2919]        4.1        5.8\nPulska et al., 1998   1.9436 [1.3441;  2.8107]        3.1        6.2\nRoberts et al., 1990  2.3010 [1.9206;  2.7567]       23.7        7.0\nSaz et al., 1999      2.1837 [1.5533;  3.0700]        5.5        6.3\nSharma et al., 1998   2.0500 [1.0744;  3.9114]        2.5        4.7\nTakeida et al., 1997  6.9784 [4.1303; 11.7902]        1.5        5.3\nTakeida et al., 1999  5.8124 [3.8816;  8.7035]        3.3        6.0\nThomas et al., 1992   1.3303 [0.7780;  2.2745]        4.0        5.3\nThomas et al., 1992   1.7722 [1.1073;  2.8363]        4.1        5.6\nWeissman et al., 1986 1.2500 [0.6678;  2.3398]        2.7        4.8\nZheng et al., 1997    1.9803 [1.4001;  2.8011]        6.4        6.3\n\nNumber of studies: k = 18\nNumber of observations: o = 94770 (o.e = 4514, o.c = 90256)\nNumber of events: e = 5439\n\n                         RR           95%-CI   z|t  p-value\nCommon effect model  2.0634 [1.8909; 2.2516] 16.26 &lt; 0.0001\nRandom effects model 2.0217 [1.5786; 2.5892]  6.00 &lt; 0.0001\n\nQuantifying heterogeneity (with 95%-CIs):\n tau^2 = 0.1865 [0.0739; 0.5568]; tau = 0.4319 [0.2718; 0.7462]\n I^2 = 77.2% [64.3%; 85.4%]; H = 2.09 [1.67; 2.62]\n\nTest of heterogeneity:\n     Q d.f.  p-value\n 74.49   17 &lt; 0.0001\n\nDetails of meta-analysis methods:\n- Mantel-Haenszel method (common effect model)\n- Inverse variance method (random effects model)\n- Paule-Mandel estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Calculation of I^2 based on Q\n- Hartung-Knapp adjustment for random effects model (df = 17)\n\n\n\nsummary(m.bin) 결과 설명\n\n\n%W(common), %W(random): 각 연구가 고정 효과 모델과 랜덤 효과 모델에서 전체 결과에 미치는 가중치를 백분율로 나타냅니다.\nNumber of studies: k = 18: 분석에 포함된 연구의 수입니다.\nNumber of observations: o = 94770 (o.e = 4514, o.c = 90256): 총 관측치 수와 실험군, 대조군의 관측치 수입니다.\nNumber of events: e = 5439: 총 사건 발생 수입니다.\nCommon effect model, Random effects model: 고정 효과 모델과 랜덤 효과 모델의 종합 위험비와 95% 신뢰구간, z/t 값, p 값을 나타냅니다.\n\nRR 2.0634 [1.8909; 2.2516]: 고정 효과 모델에서의 종합 위험비는 2.06이며, 95% 신뢰구간은 1.89에서 2.25입니다. 이는 우울증 그룹의 사망 위험이 비우울증 그룹보다 약 2.06배 높다는 것을 의미합니다.\nRR 2.0217 [1.5786; 2.5892]: 랜덤 효과 모델에서의 종합 위험비는 2.02이며, 95% 신뢰구간은 1.58에서 2.59입니다.\nz|t, p-value: 통계적 유의성을 나타냅니다. p 값이 0.0001보다 작으므로 통계적으로 유의미한 결과를 보여줍니다.\n\nQuantifying heterogeneity: 연구 간 이질성을 나타냅니다.\n\ntau^2, tau: 연구 간 분산과 표준편차를 나타냅니다.\nI^2: 이질성의 정도를 백분율로 나타냅니다. 77.2%는 높은 이질성을 의미합니다.\n\nI² (I-squared): 이질성 정도\n\n의미: 총 관측된 이질성 중 실제 이질성이 차지하는 비율을 백분율로 나타냅니다.\n해석:\n\n0~25%: 낮은 이질성\n25~50%: 중간 정도의 이질성\n50~75%: 높은 이질성\n75% 이상: 매우 높은 이질성 주어진 결과: I² = 77.2%이므로, 연구들 간에 매우 높은 이질성이 존재함을 나타냅니다.\n\n\n\nH: 이질성의 정도를 나타내는 또 다른 지표입니다.\n\nTest of heterogeneity: 이질성의 통계적 유의성을 검정합니다.\n\nQ, d.f., p-value: 이질성 검정의 통계량, 자유도, p 값을 나타냅니다. p 값이 0.0001보다 작으므로 이질성이 통계적으로 유의미합니다.\n\nDetails of meta-analysis methods: 사용된 메타 분석 방법의 세부 정보를 나타냅니다.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>메타분석</span>"
    ]
  },
  {
    "objectID": "MetaAnalysis.html#summarym.bin-결과-설명",
    "href": "MetaAnalysis.html#summarym.bin-결과-설명",
    "title": "2  메타분석",
    "section": "2.5 summary(m.bin) 결과 설명",
    "text": "2.5 summary(m.bin) 결과 설명\n\nReview: Depression and Mortality: 메타 분석의 제목입니다.\nRR, 95%-CI: 각 연구의 위험비와 95% 신뢰구간을 나타냅니다.\n%W(common), %W(random): 각 연구가 고정 효과 모델과 랜덤 효과 모델에서 전체 결과에 미치는 가중치를 백분율로 나타냅니다.\nNumber of studies: k = 18: 분석에 포함된 연구의 수입니다.\nNumber of observations: o = 94770 (o.e = 4514, o.c = 90256): 총 관측치 수와 실험군, 대조군의 관측치 수입니다.\nNumber of events: e = 5439: 총 사건 발생 수입니다.\nCommon effect model, Random effects model: 고정 효과 모델과 랜덤 효과 모델의 종합 위험비와 95% 신뢰구간, z/t 값, p 값을 나타냅니다.\n\nRR 2.0634 [1.8909; 2.2516]: 고정 효과 모델에서의 종합 위험비는 2.06이며, 95% 신뢰구간은 1.89에서 2.25입니다. 이는 우울증 그룹의 사망 위험이 비우울증 그룹보다 약 2.06배 높다는 것을 의미합니다.\nRR 2.0217 [1.5786; 2.5892]: 랜덤 효과 모델에서의 종합 위험비는 2.02이며, 95% 신뢰구간은 1.58에서 2.59입니다.\nz|t, p-value: 통계적 유의성을 나타냅니다. p 값이 0.0001보다 작으므로 통계적으로 유의미한 결과를 보여줍니다.\n\nQuantifying heterogeneity: 연구 간 이질성을 나타냅니다.\n\ntau^2, tau: 연구 간 분산과 표준편차를 나타냅니다.\nI^2: 이질성의 정도를 백분율로 나타냅니다. 77.2%는 높은 이질성을 의미합니다.\nH: 이질성의 정도를 나타내는 또 다른 지표입니다.\n\nTest of heterogeneity: 이질성의 통계적 유의성을 검정합니다.\n\nQ, d.f., p-value: 이질성 검정의 통계량, 자유도, p 값을 나타냅니다. p 값이 0.0001보다 작으므로 이질성이 통계적으로 유의미합니다.\n\nDetails of meta-analysis methods: 사용된 메타 분석 방법의 세부 정보를 나타냅니다.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>메타분석</span>"
    ]
  },
  {
    "objectID": "timeseries.html",
    "href": "timeseries.html",
    "title": "3  시계열분석",
    "section": "",
    "text": "3.1 introduction\n시계열적 자료를 분석하고 나타나는 현상과 특정 요인과 관련성을 탐색해보는 시간입니다. 예를 들어 미세먼지가 높은 날 심혈관 질환이 발생하는가?에 대한 질문에 답하기 위해서 생가할 것이 몇가지 있습니다. 미세먼지가 높은 날이란? 심혈관 질환 사망이 높은 날이란? 이 두가지 요소를 검토하게 됩니다.  그런데 심혈관 질환의 사망은 요일마다 다르고, 계절에 따라 변동하며, 장기 적으로는 점차 증가 또는 감소를 합니다. 그런데 미세먼지도 점차 증가하고 있으니, 단순 상관관계를 보면 미세먼지도 증가 심혈관 사망도 증가하면 양의 관련성을 보이게 됩니다. 마찬가지로 GDP와 자살의 관계를 보면 어떨까요? 우리나라의 자살률은 증가하고 있습니다. 그런데 GDP도 증가하고 있습니다. 그러니 GDP의 증가와 자살의 증가는 양의 상관관계가 있다고 나옵니다. 맞나요? 네 심혈관 사망, 자살의 증가의 계절적 요소, 장기간 추세(trend)가 아니라 변동이 미세먼지나 GDP의 변동과 어떠한 관계가 있는지가 우리의 궁금증일 것 입니다. 이러한 궁금증을 R을 이용해서 풀어보도록 하겠습니다.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>시계열분석</span>"
    ]
  },
  {
    "objectID": "timeseries.html#a-case-study-in-air-pollution-and-health",
    "href": "timeseries.html#a-case-study-in-air-pollution-and-health",
    "title": "3  시계열분석",
    "section": "3.2 A case study in Air Pollution and Health",
    "text": "3.2 A case study in Air Pollution and Health\nthe original book is https://www.springer.com/gp/book/9780387781662 이 책에서 중요한 부분을 요약하고, 몇몇을 추가하여 진행하겠습니다. 원본을 읽어 보시는 것을 추천해 드립니다.\n\n3.2.1 Starting Up\n이 책은 NMMAPSlite 패키지를 사용했지만, 해당 패키지와 데이터는 사용하기 쉽지 않습니다. 따라서 Gasparrini의 dlnm 패키지와 해당 패키지의 데이터가 이 요약 튜토리얼(https://github.com/gasparrini/dlnm/tree/master/data)에서 사용되었습니다.\n필요한 라이브러리를 불러옵니다.\n\nif(!require(tidyverse)) install.packages(\"tidyverse\")\nif(!require(dlnm)) install.packages(\"dlnm\")\nif(!require(ggplot2)) install.packages(\"ggplot2\")\nif(!require(readxl)) install.packages(\"readxl\")\nif(!require(lubridate)) install.packages(\"lubridate\")\nif(!require(plotly)) install.packages(\"plotly\")\nif(!require(forecast)) install.packages(\"forecast\")\nif(!require(gsheet)) install.packages(\"gsheet\")\nif(!require(Hmisc)) install.packages(\"Hmisc\")\nif(!require(mgcv)) install.packages(\"mgcv\")\n\n\n\n3.2.2 Statistical Issues in Estimating the Health Effects of Spatial–Temporal Environmental Exposures\n\n3.2.2.1 tell a story\n’매일 발생하는 대기 오염 수준의 변화와 매일 발생하는 사망자 수 변화 사이의 관계’에 대한 이야기를 하고 싶습니다. 따라서, 예측보다는 연관성을 추정하는 데 유용한 통계 모델이 필요합니다.\n\n\n3.2.2.2 Estimation Vs. Predictiion\nOne question of scientific interest might be, “Are changes in the PM10 series associated with changes in the mortality series?” This question is fundamentally about the relationship between a time-varying health outcome \\(y_{t}\\) and a time-varying exposure \\(x_{t}\\). A simple linear model might relate\n\\[Y_{i}=\\beta_{0}+\\beta_{1}\\textrm{x}_{t}+\\epsilon_{t} \\tag{1.1}\\]\n\n\n\n\n\n\n\nestimates\ncontents\n\n\n\n\n\\(\\beta_{0}\\)\nthe mean mortality count\n\n\n\\(\\beta_{1}\\)\ntthe increase in mortality associated with a unit increase in PM10(\\(x_{t}\\))\n\n\n\\(\\epsilon_{t}\\)\na stationary mean zero error process.\n\n\n\nFor example, suppose we took the exposure series \\(x_{t}\\) and decomposed it into two parts, average(\\(\\bar{x}_{t}^{Y}\\)) + deviation(\\(x_{t} - \\bar{x}_{t}^{Y}\\))\naverage: \\(\\bar{x}_{t}^{Y}\\) deviation: \\(x_{t} - \\bar{x}_{t}^{Y}\\)\nSo, we can reformulate (1.1) as below\n\\[Y_{t}=\\beta_{0}+\n\\beta_{1}\\bar{x}_{t}^{Y}+\\beta_{2}(x_{t} - \\bar{x}_{t}^{Y})\n+\\epsilon_{t} \\tag{1.2}\\]\nNote that model (1.2) is equivalent to model (1.1) if β1 = β2, however, model (1.2) does not require them to be equal.\nIn the same context, the yearly average can be decomposing seasonal average or monthly average (\\(z_{t}\\)) \\[z_{t} = \\bar{z}_{t}^{S}+(z_{t} - \\bar{z}_{t}^{S}) \\tag{1.3}\\] So, we can use following model\n\\[Y_{t}=\\beta_{0}+\n\\beta_{1}\\bar{x}_{t}^{Y}+\\beta_{2}\\bar{z}_{t}^{S}+\\beta_{3}(z_{t} - \\bar{z}_{t}^{S})\n+\\epsilon_{t} \\tag{1.2}\\]\nGoing to step further, we can add or decompose weekly moving average (\\(u_{t}\\)) \\[u_{t} = \\bar{u}_{t}^{W}+(u_{t} - \\bar{u}_{t}^{W}) \\tag{1.4}\\]\nLet, residual variation (\\(r_{t}\\)) as \\(r_{t} = (u_{t} - \\bar{u}_{t}^{W})\\). then our expanded model is now\n\\[Y_{t}=\\beta_{0}+\n\\beta_{1}\\bar{x}_{t}^{Y}+\\beta_{2}\\bar{z}_{t}^{S}+\\beta_{3}\\bar{u}_{t}^W +\\beta_{4}r_{t}\n+\\epsilon_{t} \\tag{1.5}\\]\nThe parameter β4 describes the association between yt and the sub-weekly fluctuations in \\(x_{t}\\) (adjusted for the yearly, seasonal, and weekly variation).\n\nQuestion & Discussion: What’s mean of \\(\\beta_{4}\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>시계열분석</span>"
    ]
  },
  {
    "objectID": "timeseries.html#simulation-study-for-pm10-and-cvd-mortality",
    "href": "timeseries.html#simulation-study-for-pm10-and-cvd-mortality",
    "title": "3  시계열분석",
    "section": "3.3 simulation study for pm10 and cvd mortality",
    "text": "3.3 simulation study for pm10 and cvd mortality\nFirst, let’s use simulation data to help us understand some time series data analysis.\n\nLet’s think of x as a day, and hypothetically create a random variable y1 and pm10 (fine dust) multiplied by 4.5 for 300 days\n\n\nset.seed(1)\nx  &lt;- 1:300\ny1 &lt;- 5*rnorm(300, sd=.1)+15\npm &lt;- y1*4.5\nplot(x, pm, type='l')\n\n\n\n\n\n\n\n\n\nHere, it is assumed that the long term trend gradually increases, and seasonal factor through the sin() function were added, and multiplying it by 0.03.\n\n\ny2 &lt;- y1*5+ sin(x/2)*5+ x * 0.03 \ny2[y2&lt; 0]&lt;-0\ny3&lt;-round(y2)\nplot(y3, type='l')\n\n\n\n\n\n\n\n\n\nI tried adding delay effects and days with specific events. And I created a dataframe.\n\n\nlag &lt;-6\nmean(y3)\n\n[1] 79.58667\n\ndeath &lt;- c(rep(c(80,79,81), (lag/3)), y3[1:(length(y3)-lag)])  \nevent &lt;- c(rep(1, 30), rep(1, 30), rep(0, 240)) \neventd &lt;- c(rep(40,30), rep(30, 30), rep(0, 240))\ndeath2&lt;-death+eventd+10\ngg &lt;- data.frame(x, pm, y3, death, event, death2) \nhead(gg)\n\n  x       pm y3 death event death2\n1 1 66.09048 76    80     1    130\n2 2 67.91320 80    79     1    129\n3 3 65.61984 78    81     1    131\n4 4 71.08938 84    80     1    130\n5 5 68.24139 79    79     1    129\n6 6 65.65395 74    81     1    131\n\n\n\nNow let’s make plot. There is an event in the first 50 days, so cardiovascular mortality is high. Then cvd is increasing slowly with a seasonal component. Fine dust was created with random + seasonal elements.\n\n\nplot(x, pm, type=\"l\", col=grey(0.5), ylim=c(50, 140), xlim=c(0, 300))\ngrid()\nlines(x, death2, col=grey(0.7), type=\"p\", cex=0.5)\n\n\n\n\n\n\n\n\n\nNow let’s do a simple regression analysis. What relationship are you observing? A lot of people die during the event. But there were no relationship between PM10 and CVD mortality. Oh~NO! It is clearly created CVD mortality data by simulation to relate with fine dust. Yes. ‘lag’ and ‘seasonality’ are not considered yet.\n\n\nmt3 &lt;- glm(death2 ~ x+sin(x/2)+pm+event)\nsummary(mt3)$coefficients\n\n               Estimate  Std. Error     t value      Pr(&gt;|t|)\n(Intercept) 90.10455149 6.861474711  13.1319513  2.476263e-31\nx            0.02379324 0.003500538   6.7970236  5.915161e-11\nsin(x/2)    -4.41585403 0.308633540 -14.3077581  1.247252e-35\npm          -0.06144597 0.101078610  -0.6079028  5.437196e-01\nevent       35.05109683 0.757861036  46.2500315 3.230388e-137\n\n\n\nlet’s review the plot, how about the fitted line?\n\n\nplot(x, pm, type=\"l\", col=grey(0.5), ylim=c(50, 140), xlim=c(0, 300))\ngrid()\nlines(x, death2, col=grey(0.7), type=\"p\", cex=0.5)\nmp3 &lt;- c( predict(mt3))\nlines(x, mp3, col=75)\n\n\n\n\n\n\n\n\n\nnow, here is simple regression model between residual of death and pm10. Is it correct?\n\n\nmt2 &lt;- glm(death2 ~ x+sin(x/2)+event)\nresid_mt2 &lt;-resid(mt2)\nrisk.m0&lt;-glm(resid_mt2 ~ pm, family=gaussian)\nsummary(risk.m0)\n\n\nCall:\nglm(formula = resid_mt2 ~ pm, family = gaussian)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  4.14114    6.79037    0.61    0.542\npm          -0.06128    0.10043   -0.61    0.542\n\n(Dispersion parameter for gaussian family taken to be 14.18007)\n\n    Null deviance: 4230.9  on 299  degrees of freedom\nResidual deviance: 4225.7  on 298  degrees of freedom\nAIC: 1650.9\n\nNumber of Fisher Scoring iterations: 2\n\nrisk.mp0 &lt;- c( predict(risk.m0))\nplot(pm, resid_mt2, type='p', cex=0.5)\nlines(pm, (risk.mp0), col=25)\n\n\n\n\n\n\n\n\n\nHere is another simple regression model between residual of death and residual of pm10. Is it correct?\n\n\nmt2 &lt;- glm(death2 ~ x+sin(x/2)+event)\nresid_mt2 &lt;-resid(mt2)\npm2 &lt;- glm(pm ~ x+sin(x/2))\nresid_pm2 &lt;-resid(pm2)\n\nrisk.m1&lt;-glm(resid_mt2 ~ resid_pm2, family=gaussian)\nrisk.mp1 &lt;- c( predict(risk.m1))\nplot(resid_pm2, resid_mt2, type='p', cex=0.5)\nlines(resid_pm2, (risk.mp0), col=25)\n\n\n\n\n\n\n\n\nThis is an intuitive graph. The relationship between two residuals after removing time trends, seasonal fluctation.\n\n3.3.0.1 Autocorrelation\n\nlibrary(tidyverse)\ndat = cbind('time'=x, pm,death2,event) %&gt;% data.frame() %&gt;% tibble()\ndat %&gt;% head()\n\n# A tibble: 6 × 4\n   time    pm death2 event\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1     1  66.1    130     1\n2     2  67.9    129     1\n3     3  65.6    131     1\n4     4  71.1    130     1\n5     5  68.2    129     1\n6     6  65.7    131     1\n\n\nAutocorrelation is the amount of association observations at a time and other observations with time lag. For example, weekend have 7 days autocorrelation, and seasons have 12 months autocorrelation.\n\\[ r(k) = \\frac{1}{N} \\sum_{t=1}^{N-k}\n(x_{t} - \\bar{x})(x_{t+k} - \\bar{x})/c(0) \\tag{2.1}  \\]\n\\[ c(0) = \\frac{1}{N} \\sum_{t=1}^{N-k}(x_{t} - \\bar{x})^2 \\]\nOne of the most important factor in autocorrelation is the seasonal factor, so compare ACF plot before and after removing the seasonal factor\n\n#par(mflow)\npar(mfrow=c(4, 2))\nplot(x, death2, type='p', cex=0.5)\nacf(dat$death2)\n# adjusting seasonality\nar1 &lt;- glm(death2 ~ x +sin(x/2)+event)\nplot(x, death2, type='p', cex=0.5)\nlines(x, predict(ar1), col = 'red')\nacf(resid(ar1))\n# adjusting seasonality by gam model\nlibrary(mgcv)\nar2 &lt;- mgcv::gam(death2 ~ s(x,bs=\"cc\", k=100)+event, family=gaussian)\nplot(x, death2, type='p', cex=0.5)\nlines(x, predict(ar2), col = 'red')\nacf(resid(ar2))\nlibrary(forecast)\nauto.arima(dat$death2)\n\nSeries: dat$death2 \nARIMA(2,1,2) \n\nCoefficients:\n         ar1      ar2      ma1     ma2\n      0.6952  -0.4906  -0.8632  0.7667\ns.e.  0.1099   0.1185   0.0810  0.0872\n\nsigma^2 = 15.81:  log likelihood = -835.21\nAIC=1680.42   AICc=1680.62   BIC=1698.92\n\nm1&lt;-arima(dat$death2, order=c(2,1,2))\nplot(x, death2, type='p', cex=0.5)\nlines(fitted(m1), col=\"red\")\nacf(resid(m1))\n\n\n\n\n\n\n\n\nHow do we remove or control autocorrelation? What model is more appropriate? if we want to tell the story that mortality rates can be changed with fluctuation of none-seasonal factors, rather than seasonal factors do, we should control or remove autocorrelation.\nIt would be nice and easy when we use ‘gam’ model. And let’s recall that we’re going to look at the relationship between residual(death) and residual(pm). There is gam model of cubic spline with 100 df. the summary results of two model is almost same, so we can use gam model of mod1 to find relasionship between two residuals.\n\nlibrary(mgcv)\ntime   = dat$time\nmod1   = mgcv::gam(death2 ~ pm + s(time, bs='cc', k=100))\nmod1 %&gt;% \n  summary()\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\ndeath2 ~ pm + s(time, bs = \"cc\", k = 100)\n\nParametric coefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 105.38002    6.56030  16.063   &lt;2e-16 ***\npm           -0.13082    0.09704  -1.348    0.179    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n          edf Ref.df     F p-value    \ns(time) 72.67     98 50.72  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.943   Deviance explained = 95.7%\nGCV = 13.955  Scale est. = 10.482    n = 300\n\nrpm    = mgcv::gam(pm ~ s(time, bs='cc', k=100))\nrdeath = mgcv::gam(death2 ~ s(time, bs='cc', k=100))\nmod2   = mgcv::gam(resid(rdeath) ~ resid(rpm))\nmod2 %&gt;% \n  summary()\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nresid(rdeath) ~ resid(rpm)\n\nParametric coefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  1.528e-14  1.626e-01   0.000    1.000\nresid(rpm)  -1.036e-01  7.513e-02  -1.379    0.169\n\n\nR-sq.(adj) =  0.00301   Deviance explained = 0.634%\nGCV = 7.9884  Scale est. = 7.9352    n = 300\n\n\n\nLag time effect is another important issue. The assumption of lag is that cvd mortality incresed after 6 day later from PM10 increment time.\n\n\nmean(pm)\n\n[1] 67.57556\n\nlag.pm&lt;-6\npm.lag &lt;- c(rep(67.5, lag.pm), pm[1:(length(pm)-lag.pm)])\nresid_mt3 &lt;-resid(mt3)\nrisk.m1&lt;-glm(resid_mt3 ~ pm.lag, family=gaussian)\nsummary(risk.m1)$coefficients\n\n              Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) -76.437599 5.21757250 -14.65003 5.620794e-37\npm.lag        1.131554 0.07720006  14.65743 5.276143e-37\n\nrisk.mp1 &lt;- c( predict(risk.m1))\nplot(pm.lag, resid_mt3, type='p', cex=0.5)\nlines(pm.lag, risk.mp1, col=25)\n\n\n\n\n\n\n\n\nOh! There is positive association between pm10 and cvd mortality.\n\nPlot highlight lag effect between cvd and pm10.\n\n\nplot(x, resid_mt3, type=\"l\", col=grey(0.5), ylim=c(-15, 40), xlim=c(0, 300))\ngrid()\nlines(x, (pm-50), col=grey(0.7), type=\"l\", cex=0.5)\nlines(x, (pm.lag-60), col='red', type=\"l\", cex=0.5)\n\n\n\n\n\n\n\n\n\nThe relationship between pm and cardiovascular death was analyzed after considering the seasonal factor with sin() and the delay effect with lag and removing time series factor(residual)\n\n\n#install.packages('mgcv')\nlibrary(mgcv)\n#library(gam)\nmgam&lt;- mgcv::gam(death2 ~ s(x, bs=\"cc\", k=100)+event, family=gaussian)\np &lt;- predict(mgam)\nplot(x, pm, type=\"l\", col=grey(0.5), ylim=c(40, 150), xlim=c(0, 300), cex=2)\ngrid()\nlines(x, death2, col=grey(0.7), type=\"p\", cex=0.5)\nlegend(x=250, y=70, 'PM10')\nlegend(x=150, y=65, 'pm10. lag')\nlegend(x=210, y=110, 'Obs_death')\nlegend(x=10, y=50, 'Residual(Obs_Death - Gam(fitting)')\nlines(x, p)\nlines(x, (resid(mgam)+50), col='blue')\nlines(x, pm.lag-10, col='red')\n\n\n\n\n\n\n\n\n\nLet’s solve this by regression analysis. What about the model as k is higher? Yes, you should consider how to choose the lag time and k values. Data driven method is common method to choose fitted model. The minimal AIC or BIC value suggest more fitted model.\n\n\nmgam&lt;- mgcv::gam(death2 ~ s(x, bs=\"cc\", k=100)+event, family=gaussian)\np &lt;- predict(mgam)\nrisk.pp1 &lt;-glm(death2 ~ p+pm.lag,family=gaussian)\nsummary(risk.pp1)$coefficients\n\n               Estimate  Std. Error   t value     Pr(&gt;|t|)\n(Intercept) -58.3872771 1.937186312 -30.14025 2.402418e-92\np             1.0000436 0.004566766 218.98286 0.000000e+00\npm.lag        0.8642815 0.028266084  30.57663 9.482839e-94\n\nAIC(risk.pp1)\n\n[1] 885.3135\n\n\n\nmgam150&lt;- mgcv::gam(death2 ~ s(x, bs=\"cc\", k=10)+event)\np150 &lt;- predict(mgam150)\nrisk.pp150 &lt;-glm(death2 ~ p150+ pm.lag, family=gaussian)\nsummary(risk.pp150)$coefficients\n\n               Estimate Std. Error   t value      Pr(&gt;|t|)\n(Intercept) -72.5953393 6.74682944 -10.75992  5.109224e-23\np150          0.9979243 0.01630871  61.18966 2.087420e-170\npm.lag        1.0776412 0.09754736  11.04736  5.349868e-24\n\nAIC(risk.pp1, risk.pp150)\n\n           df       AIC\nrisk.pp1    4  885.3135\nrisk.pp150  4 1629.2747\n\n\n\nLet’s find the lag using dlnm packages (distributed lag non-linear model)\n\n\nlibrary(dlnm)\ncb1.pm &lt;-crossbasis(pm, lag=10, argvar=list(fun=\"lin\"),\n     arglag=list(fun=\"poly\", degree=3))\nmodel1 &lt;-glm(death2 ~ cb1.pm+x+event , \n              family=gaussian )\npred1.pm &lt;-crosspred(cb1.pm, model1, at=0:100, bylag=0.1, cumul=TRUE)\n\nplot(pred1.pm, \"slices\", var=1, col=3, ylab=\"RR\", ci.arg=list(density=15,lwd=2),\n     #cumul = TRUE,\nmain=\"Association with a 1-unit increase in PM10\")\n\n\n\n\n\n\n\n\n\nThe \\(\\beta\\) is highest at 6 days lag.\nNow we know we can do a regression analysis with 6 days as the lag time. All that’s left is to discuss further how to find time-series elements, how to correct them, and how to rationalize this process.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>시계열분석</span>"
    ]
  },
  {
    "objectID": "timeseries_1.html",
    "href": "timeseries_1.html",
    "title": "3  시계열분석",
    "section": "",
    "text": "3.1 introduction\n시계열적 자료를 분석하고 나타나는 현상과 특정 요인과 관련성을 탐색해보는 시간입니다. 예를 들어 미세먼지가 높은 날 심혈관 질환이 발생하는가?에 대한 질문에 답하기 위해서 생가할 것이 몇가지 있습니다. 미세먼지가 높은 날이란? 심혈관 질환 사망이 높은 날이란? 이 두가지 요소를 검토하게 됩니다.  그런데 심혈관 질환의 사망은 요일마다 다르고, 계절에 따라 변동하며, 장기 적으로는 점차 증가 또는 감소를 합니다. 그런데 미세먼지도 점차 증가하고 있으니, 단순 상관관계를 보면 미세먼지도 증가 심혈관 사망도 증가하면 양의 관련성을 보이게 됩니다. 마찬가지로 GDP와 자살의 관계를 보면 어떨까요? 우리나라의 자살률은 증가하고 있습니다. 그런데 GDP도 증가하고 있습니다. 그러니 GDP의 증가와 자살의 증가는 양의 상관관계가 있다고 나옵니다. 맞나요? 네 심혈관 사망, 자살의 증가의 계절적 요소, 장기간 추세(trend)가 아니라 변동이 미세먼지나 GDP의 변동과 어떠한 관계가 있는지가 우리의 궁금증일 것 입니다. 이러한 궁금증을 R을 이용해서 풀어보도록 하겠습니다.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>시계열분석</span>"
    ]
  },
  {
    "objectID": "timeseries_1.html#공기-오염와-건강",
    "href": "timeseries_1.html#공기-오염와-건강",
    "title": "3  시계열분석",
    "section": "3.2 공기 오염와 건강",
    "text": "3.2 공기 오염와 건강\nthe original book is https://www.springer.com/gp/book/9780387781662 이 책에서 중요한 부분을 요약하고, 몇몇을 추가하여 진행하겠습니다. 원본을 읽어 보시는 것을 추천해 드립니다.\n\n3.2.1 시작\n이 책은 NMMAPSlite 패키지를 사용했지만, 해당 패키지와 데이터는 사용하기 쉽지 않습니다. 따라서 Gasparrini의 dlnm 패키지와 해당 패키지의 데이터가 이 요약 튜토리얼(https://github.com/gasparrini/dlnm/tree/master/data)에서 사용되었습니다.\n필요한 라이브러리를 불러옵니다.\n\nif(!require(tidyverse)) install.packages(\"tidyverse\")\nif(!require(dlnm)) install.packages(\"dlnm\")\nif(!require(ggplot2)) install.packages(\"ggplot2\")\nif(!require(readxl)) install.packages(\"readxl\")\nif(!require(lubridate)) install.packages(\"lubridate\")\nif(!require(plotly)) install.packages(\"plotly\")\nif(!require(forecast)) install.packages(\"forecast\")\nif(!require(gsheet)) install.packages(\"gsheet\")\nif(!require(Hmisc)) install.packages(\"Hmisc\")\nif(!require(mgcv)) install.packages(\"mgcv\")\n\n\n\n3.2.2 환경오염과 건강을 어떻게 연구할 것인가가\n\n3.2.2.1 tell a story\n’매일 발생하는 대기 오염 수준의 변화와 매일 발생하는 사망자 수 변화 사이의 관계’에 대한 이야기를 하고 싶습니다. 따라서, 예측보다는 연관성을 추정하는 데 유용한 통계 모델이 필요합니다.\n\n\n3.2.2.2 추정 vs. 예측\n과학적으로 흥미로운 질문 중 하나는 “PM10 시계열의 변화가 사망률 시계열의 변화와 관련이 있는가?”입니다. 이 질문은 본질적으로 시간에 따라 변하는 건강 결과 \\(y_{t}\\)와 시간에 따라 변하는 노출 \\(x_{t}\\) 간의 관계를 탐구합니다. 이를 간단한 선형 모델로 표현하면 (식 1.1)과 같이 나타낼 수 있습니다.\n\\[Y_{i}=\\beta_{0}+\\beta_{1}\\textrm{x}_{t}+\\epsilon_{t} \\tag{1.1}\\]\n\n\n\n\n\n\n\nestimates\ncontents\n\n\n\n\n\\(\\beta_{0}\\)\nthe mean mortality count\n\n\n\\(\\beta_{1}\\)\ntthe increase in mortality associated with a unit increase in PM10(\\(x_{t}\\))\n\n\n\\(\\epsilon_{t}\\)\na stationary mean zero error process.\n\n\n\n\\(\\bar{x}_{t}^{Y}\\): Y라는 특정 변수의 t 시점에서의 평균값을 의미합니다. \\(\\bar{x}_{t}\\): t 시점에서의 Y 변수의 실제 값을 의미합니다.\n예를 들어, 노출 시계열 \\(x_{t}\\)를 가져와서 두 부분으로 분해한다고 가정해 봅시다: 평균(\\(\\bar{x}{t}^{Y}\\)) + 편차(\\(x{t} - \\bar{x}_{t}^{Y}\\))\naverage: \\(\\bar{x}_{t}^{Y}\\)\ndeviation: \\(x_{t} - \\bar{x}_{t}^{Y}\\)\n따라서 우리는 (1.1)을 아래와 같이 재구성할 수 있습니다:\n\\[Y_{t}=\\beta_{0}+\n\\beta_{1}\\bar{x}_{t}^{Y}+\\beta_{2}(x_{t} - \\bar{x}_{t}^{Y})\n+\\epsilon_{t} \\tag{1.2}\\]\n모델 (1.2)는 \\(\\beta_1 = \\beta_2\\)라면 모델 (1.1)과 동일하지만, 모델 (1.2)는 이 둘이 같을 필요는 없습니다.\n같은 맥락에서, 연간 평균은 계절 평균 또는 월간 평균(\\(z_{t}\\))으로 분해될 수 있습니다.\n\\[z_{t} = \\bar{z}_{t}^{S}+(z_{t} - \\bar{z}_{t}^{S}) \\tag{1.3}\\] So, we can use following model\n\\[Y_{t}=\\beta_{0}+\n\\beta_{1}\\bar{x}_{t}^{Y}+\\beta_{2}\\bar{z}_{t}^{S}+\\beta_{3}(z_{t} - \\bar{z}_{t}^{S})\n+\\epsilon_{t} \\tag{1.2}\\]\n한 단계 더 나아가, 주간 이동 평균(\\(u_{t}\\))을 추가하거나 분해할 수 있습니다: \\[u_{t} = \\bar{u}_{t}^{W}+(u_{t} - \\bar{u}_{t}^{W}) \\tag{1.4}\\]\n잔차 변동(\\(r_{t}\\))을 \\(r_{t} = (u_{t} - \\bar{u}_{t}^{W})\\)로 정의하면, 확장된 모델은 이제 다음과 같습니다:\n\\[Y_{t}=\\beta_{0}+\n\\beta_{1}\\bar{x}_{t}^{Y}+\\beta_{2}\\bar{z}_{t}^{S}+\\beta_{3}\\bar{u}_{t}^W +\\beta_{4}r_{t}\n+\\epsilon_{t} \\tag{1.5}\\]\n매개변수 \\(\\beta_{4}\\)는 \\(Y_{t}\\)와 \\(x_{t}\\)의 주간 이하 변동 사이의 연관성을 설명합니다 (연간, 계절, 주간 변동을 조정한 후).\n\n질문 및 토론: \\(\\beta_{4}\\)의 의미는 무엇인가요?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>시계열분석</span>"
    ]
  },
  {
    "objectID": "timeseries_1.html#pm10과-심혈관-질환-사망률에-대한-시뮬레이션-연구",
    "href": "timeseries_1.html#pm10과-심혈관-질환-사망률에-대한-시뮬레이션-연구",
    "title": "3  시계열분석",
    "section": "3.3 PM10과 심혈관 질환 사망률에 대한 시뮬레이션 연구",
    "text": "3.3 PM10과 심혈관 질환 사망률에 대한 시뮬레이션 연구\n먼저, 시뮬레이션 데이터를 사용하여 시계열 데이터 분석을 이해해보겠습니다.\n\nx를 날짜로 생각하고, 가상으로 300일 동안 랜덤 변수 y1과 PM10(미세먼지)을 4.5배 곱한 값을 생성해봅시다.\n\n\nset.seed(1)\nx  &lt;- 1:300\ny1 &lt;- 5*rnorm(300, sd=.1)+15\npm &lt;- y1*4.5\nplot(x, pm, type='l')\n\n\n\n\n\n\n\n\n\n여기서는 장기적인 추세가 점진적으로 증가한다고 가정하고, sin() 함수를 통해 계절적 요인을 추가했으며, 이를 0.03으로 곱했습니다.\n\n\ny2 &lt;- y1*5+ sin(x/2)*5+ x * 0.03 \ny2[y2&lt; 0]&lt;-0\ny3&lt;-round(y2)\nplot(y3, type='l')\n\n\n\n\n\n\n\n\n\n지연 효과와 특정 이벤트가 있는 날을 추가해봤습니다. 그리고 데이터프레임을 만들었습니다\n\n\nlag &lt;-6\nmean(y3)\n\n[1] 79.58667\n\ndeath &lt;- c(rep(c(80,79,81), (lag/3)), y3[1:(length(y3)-lag)])  \nevent &lt;- c(rep(1, 30), rep(1, 30), rep(0, 240)) \neventd &lt;- c(rep(40,30), rep(30, 30), rep(0, 240))\ndeath2&lt;-death+eventd+10\ngg &lt;- data.frame(x, pm, y3, death, event, death2) \nhead(gg)\n\n  x       pm y3 death event death2\n1 1 66.09048 76    80     1    130\n2 2 67.91320 80    79     1    129\n3 3 65.61984 78    81     1    131\n4 4 71.08938 84    80     1    130\n5 5 68.24139 79    79     1    129\n6 6 65.65395 74    81     1    131\n\n\n\n이제 그래프를 그려봅시다. 처음 50일 동안 이벤트가 있어서 심혈관 사망률이 높습니다. 그 후 심혈관 질환 사망률은 계절적 요소와 함께 천천히 증가합니다. 미세먼지는 랜덤 + 계절적 요소로 생성되었습니다.\n\n\nplot(x, pm, type=\"l\", col=grey(0.5), ylim=c(50, 140), xlim=c(0, 300))\ngrid()\nlines(x, death2, col=grey(0.7), type=\"p\", cex=0.5)\n\n\n\n\n\n\n\n\n\n이제 간단한 회귀 분석을 해봅시다. 어떤 관계를 관찰할 수 있나요? 이벤트 기간 동안 많은 사람이 사망했습니다. 하지만 PM10과 심혈관 질환 사망률 사이에는 관계가 없어 보입니다.\n\n분명히 미세먼지와 관련되도록 시뮬레이션으로 만든 심혈관 질환 사망률 데이터인데요. 왜 그럴까요? 아직 ’지연(lag)’과 ’계절성(seasonality)’이 고려되지 않았습니다\n\nmt3 &lt;- glm(death2 ~ x+sin(x/2)+pm+event)\nsummary(mt3)$coefficients\n\n               Estimate  Std. Error     t value      Pr(&gt;|t|)\n(Intercept) 90.10455149 6.861474711  13.1319513  2.476263e-31\nx            0.02379324 0.003500538   6.7970236  5.915161e-11\nsin(x/2)    -4.41585403 0.308633540 -14.3077581  1.247252e-35\npm          -0.06144597 0.101078610  -0.6079028  5.437196e-01\nevent       35.05109683 0.757861036  46.2500315 3.230388e-137\n\n\n\n그래프를 다시 보고, 적합된 선(fitted line)은 어떠한가요?\n\n\nplot(x, pm, type=\"l\", col=grey(0.5), ylim=c(50, 140), xlim=c(0, 300))\ngrid()\nlines(x, death2, col=grey(0.7), type=\"p\", cex=0.5)\nmp3 &lt;- c( predict(mt3))\nlines(x, mp3, col=75)\n\n\n\n\n\n\n\n\n\n이제 사망률 잔차와 PM10 간의 간단한 회귀 모델입니다. 이게 맞나요?\n\n\nmt2 &lt;- glm(death2 ~ x+sin(x/2)+event)\nresid_mt2 &lt;-resid(mt2)\nrisk.m0&lt;-glm(resid_mt2 ~ pm, family=gaussian)\nsummary(risk.m0)\n\n\nCall:\nglm(formula = resid_mt2 ~ pm, family = gaussian)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  4.14114    6.79037    0.61    0.542\npm          -0.06128    0.10043   -0.61    0.542\n\n(Dispersion parameter for gaussian family taken to be 14.18007)\n\n    Null deviance: 4230.9  on 299  degrees of freedom\nResidual deviance: 4225.7  on 298  degrees of freedom\nAIC: 1650.9\n\nNumber of Fisher Scoring iterations: 2\n\nrisk.mp0 &lt;- c( predict(risk.m0))\nplot(pm, resid_mt2, type='p', cex=0.5)\nlines(pm, (risk.mp0), col=25)\n\n\n\n\n\n\n\n\n\n여기 또 다른 사망률 잔차와 PM10 잔차 간의 간단한 회귀 모델이 있습니다. 이게 맞나요?\n\n\nmt2 &lt;- glm(death2 ~ x+sin(x/2)+event)\nresid_mt2 &lt;-resid(mt2)\npm2 &lt;- glm(pm ~ x+sin(x/2))\nresid_pm2 &lt;-resid(pm2)\n\nrisk.m1&lt;-glm(resid_mt2 ~ resid_pm2, family=gaussian)\nrisk.mp1 &lt;- c( predict(risk.m1))\nplot(resid_pm2, resid_mt2, type='p', cex=0.5)\nlines(resid_pm2, (risk.mp0), col=25)\n\n\n\n\n\n\n\n\n이것은 직관적인 그래프입니다. 시간 추세와 계절적 변동을 제거한 후 두 잔차 간의 관계를 보여줍니다.\n\n3.3.0.1 자기상관 (Autocorrelation)\n\nlibrary(tidyverse)\ndat = cbind('time'=x, pm,death2,event) %&gt;% data.frame() %&gt;% tibble()\ndat %&gt;% head()\n\n# A tibble: 6 × 4\n   time    pm death2 event\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1     1  66.1    130     1\n2     2  67.9    129     1\n3     3  65.6    131     1\n4     4  71.1    130     1\n5     5  68.2    129     1\n6     6  65.7    131     1\n\n\n자기상관(Autocorrelation)은 특정 시점의 관측값이 시간 지연(time lag)을 가진 다른 관측값과 얼마나 연관되어 있는지를 나타냅니다. 예를 들어, 주말은 7일의 자기상관을 가지며, 계절은 12개월의 자기상관을 가집니다.\n\\[ r(k) = \\frac{1}{N} \\sum_{t=1}^{N-k}\n(x_{t} - \\bar{x})(x_{t+k} - \\bar{x})/c(0) \\tag{2.1}  \\]\n\\[ c(0) = \\frac{1}{N} \\sum_{t=1}^{N-k}(x_{t} - \\bar{x})^2 \\]\n여기서:\n\n\\({r}(k)\\): 시간 지연 k에서의 자기상관계수\nN: 전체 관측값의 개수\n\\({x}_{t}\\): 시점 t에서의 관측값\n\\(\\bar{x}\\): 관측값의 평균\n\\(c(0)\\): 분산\n\n자기상관 분석에서 가장 중요한 요소 중 하나는 계절성(seasonal factor)입니다. 따라서 계절성 요인을 제거하기 전과 후의 자기상관 함수(ACF) 플롯을 비교합니다.\n\n#par(mflow)\npar(mfrow=c(4, 2))\nplot(x, death2, type='p', cex=0.5)\nacf(dat$death2)\n# adjusting seasonality\nar1 &lt;- glm(death2 ~ x +sin(x/2)+event)\nplot(x, death2, type='p', cex=0.5)\nlines(x, predict(ar1), col = 'red')\nacf(resid(ar1))\n# adjusting seasonality by gam model\nlibrary(mgcv)\nar2 &lt;- mgcv::gam(death2 ~ s(x,bs=\"cc\", k=100)+event, family=gaussian)\nplot(x, death2, type='p', cex=0.5)\nlines(x, predict(ar2), col = 'red')\nacf(resid(ar2))\nlibrary(forecast)\nauto.arima(dat$death2)\n\nSeries: dat$death2 \nARIMA(2,1,2) \n\nCoefficients:\n         ar1      ar2      ma1     ma2\n      0.6952  -0.4906  -0.8632  0.7667\ns.e.  0.1099   0.1185   0.0810  0.0872\n\nsigma^2 = 15.81:  log likelihood = -835.21\nAIC=1680.42   AICc=1680.62   BIC=1698.92\n\nm1&lt;-arima(dat$death2, order=c(2,1,2))\nplot(x, death2, type='p', cex=0.5)\nlines(fitted(m1), col=\"red\")\nacf(resid(m1))\n\n\n\n\n\n\n\n\n자기상관을 제거하거나 제어하는 방법은 무엇이며, 어떤 모델이 더 적절할까요? 계절적 요인보다는 비계절적 요인의 변동에 따라 사망률이 변할 수 있다는 이야기를 하고 싶다면, 자기상관을 제어하거나 제거해야 합니다.\n‘gam’ 모델을 사용하면 쉽고 효과적입니다. 그리고 residual(death)와 residual(pm) 사이의 관계를 살펴볼 것이라는 점을 기억합시다. 자유도(df)가 100인 큐빅 스플라인(cubic spline)의 GAM 모델이 있습니다. 두 모델의 요약 결과는 거의 동일하므로, mod1의 GAM 모델을 사용하여 두 잔차 사이의 관계를 찾을 수 있습니다.\n\nlibrary(mgcv)\ntime   = dat$time\nmod1   = mgcv::gam(death2 ~ pm + s(time, bs='cc', k=100))\nmod1 %&gt;% \n  summary()\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\ndeath2 ~ pm + s(time, bs = \"cc\", k = 100)\n\nParametric coefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 105.38002    6.56030  16.063   &lt;2e-16 ***\npm           -0.13082    0.09704  -1.348    0.179    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n          edf Ref.df     F p-value    \ns(time) 72.67     98 50.72  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.943   Deviance explained = 95.7%\nGCV = 13.955  Scale est. = 10.482    n = 300\n\nrpm    = mgcv::gam(pm ~ s(time, bs='cc', k=100))\nrdeath = mgcv::gam(death2 ~ s(time, bs='cc', k=100))\nmod2   = mgcv::gam(resid(rdeath) ~ resid(rpm))\nmod2 %&gt;% \n  summary()\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nresid(rdeath) ~ resid(rpm)\n\nParametric coefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  1.528e-14  1.626e-01   0.000    1.000\nresid(rpm)  -1.036e-01  7.513e-02  -1.379    0.169\n\n\nR-sq.(adj) =  0.00301   Deviance explained = 0.634%\nGCV = 7.9884  Scale est. = 7.9352    n = 300\n\n\n지연 시간 효과(Lag time effect)는 또 다른 중요한 문제입니다. 지연 시간의 가정은 PM10 증가 시간으로부터 6일 후에 심혈관 질환 사망률이 증가한다는 것입니다.\n\nmean(pm)\n\n[1] 67.57556\n\nlag.pm&lt;-6\npm.lag &lt;- c(rep(67.5, lag.pm), pm[1:(length(pm)-lag.pm)])\nresid_mt3 &lt;-resid(mt3)\nrisk.m1&lt;-glm(resid_mt3 ~ pm.lag, family=gaussian)\nsummary(risk.m1)$coefficients\n\n              Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) -76.437599 5.21757250 -14.65003 5.620794e-37\npm.lag        1.131554 0.07720006  14.65743 5.276143e-37\n\nrisk.mp1 &lt;- c( predict(risk.m1))\nplot(pm.lag, resid_mt3, type='p', cex=0.5)\nlines(pm.lag, risk.mp1, col=25)\n\n\n\n\n\n\n\n\nPM10과 심혈관 질환 사망률 사이에 양의 연관성이 있습니다.\n심혈관 질환과 PM10 사이의 지연 효과를 강조하는 플롯입니다.\n\nplot(x, resid_mt3, type=\"l\", col=grey(0.5), ylim=c(-15, 40), xlim=c(0, 300))\ngrid()\nlines(x, (pm-50), col=grey(0.7), type=\"l\", cex=0.5)\nlines(x, (pm.lag-60), col='red', type=\"l\", cex=0.5)\n\n\n\n\n\n\n\n\nsin()을 사용하여 계절적 요인을 고려하고 lag를 사용하여 지연 효과를 고려하고 시계열 요인(잔차)을 제거한 후 PM과 심혈관 질환 사망 사이의 관계를 분석했습니다.\n\n#install.packages('mgcv')\nlibrary(mgcv)\n#library(gam)\nmgam&lt;- mgcv::gam(death2 ~ s(x, bs=\"cc\", k=100)+event, family=gaussian)\np &lt;- predict(mgam)\nplot(x, pm, type=\"l\", col=grey(0.5), ylim=c(40, 150), xlim=c(0, 300), cex=2)\ngrid()\nlines(x, death2, col=grey(0.7), type=\"p\", cex=0.5)\nlegend(x=250, y=70, 'PM10')\nlegend(x=150, y=65, 'pm10. lag')\nlegend(x=210, y=110, 'Obs_death')\nlegend(x=10, y=50, 'Residual(Obs_Death - Gam(fitting)')\nlines(x, p)\nlines(x, (resid(mgam)+50), col='blue')\nlines(x, pm.lag-10, col='red')\n\n\n\n\n\n\n\n\n회귀 분석으로 이 문제를 해결해 봅시다. k가 더 높을 때 모델은 어떻습니까? 예, 지연 시간과 k 값을 선택하는 방법을 고려해야 합니다. 데이터 기반 방법은 적합한 모델을 선택하는 일반적인 방법입니다. 최소 AIC 또는 BIC 값은 더 적합한 모델을 제시합니다.\n\nmgam&lt;- mgcv::gam(death2 ~ s(x, bs=\"cc\", k=100)+event, family=gaussian)\np &lt;- predict(mgam)\nrisk.pp1 &lt;-glm(death2 ~ p+pm.lag,family=gaussian)\nsummary(risk.pp1)$coefficients\n\n               Estimate  Std. Error   t value     Pr(&gt;|t|)\n(Intercept) -58.3872771 1.937186312 -30.14025 2.402418e-92\np             1.0000436 0.004566766 218.98286 0.000000e+00\npm.lag        0.8642815 0.028266084  30.57663 9.482839e-94\n\nAIC(risk.pp1)\n\n[1] 885.3135\n\n\n\nmgam150&lt;- mgcv::gam(death2 ~ s(x, bs=\"cc\", k=10)+event)\np150 &lt;- predict(mgam150)\nrisk.pp150 &lt;-glm(death2 ~ p150+ pm.lag, family=gaussian)\nsummary(risk.pp150)$coefficients\n\n               Estimate Std. Error   t value      Pr(&gt;|t|)\n(Intercept) -72.5953393 6.74682944 -10.75992  5.109224e-23\np150          0.9979243 0.01630871  61.18966 2.087420e-170\npm.lag        1.0776412 0.09754736  11.04736  5.349868e-24\n\nAIC(risk.pp1, risk.pp150)\n\n           df       AIC\nrisk.pp1    4  885.3135\nrisk.pp150  4 1629.2747\n\n\ndlnm 패키지(분산 지연 비선형 모델)를 사용하여 지연 시간을 찾아봅시다.\n\nlibrary(dlnm)\ncb1.pm &lt;-crossbasis(pm, lag=10, argvar=list(fun=\"lin\"),\n     arglag=list(fun=\"poly\", degree=3))\nmodel1 &lt;-glm(death2 ~ cb1.pm+x+event , \n              family=gaussian )\npred1.pm &lt;-crosspred(cb1.pm, model1, at=0:100, bylag=0.1, cumul=TRUE)\n\nplot(pred1.pm, \"slices\", var=1, col=3, ylab=\"RR\", ci.arg=list(density=15,lwd=2),\n     #cumul = TRUE,\nmain=\"Association with a 1-unit increase in PM10\")\n\n\n\n\n\n\n\n\n\\(\\beta\\)는 6일 지연에서 가장 높습니다.\n이제 6일을 지연 시간으로 사용하여 회귀 분석을 수행할 수 있다는 것을 알았습니다. 남은 것은 시계열 요소를 찾고, 수정하고, 이 과정을 합리화하는 방법에 대해 더 논의하는 것입니다.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>시계열분석</span>"
    ]
  },
  {
    "objectID": "timeseries_2.html",
    "href": "timeseries_2.html",
    "title": "4  case study: influenza epidemic and suicide",
    "section": "",
    "text": "This time, let’s talk about the flu epidemic and suicide. Suicide during flu treatment was on the news in Japan several years ago, and we would like to analyze whether it is a matter of the seasonal factor, which is the season when the flu is mainly prevalent, or whether suicide occurs when there is a really big flu epidemic.\n\n4.0.1 실습 데이터\n첫번째 실습 데이터는 감염병 포탈의 인플루엔자 자료입니다. 여기서 다운로드 합니다.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>case study: influenza epidemic and suicide</span>"
    ]
  },
  {
    "objectID": "timeseries_1.html#사례-연구-독감과-자살",
    "href": "timeseries_1.html#사례-연구-독감과-자살",
    "title": "3  시계열분석",
    "section": "3.4 사례 연구 : 독감과 자살",
    "text": "3.4 사례 연구 : 독감과 자살\n이번에는 독감 유행과 자살에 대해 이야기해 보겠습니다. 몇 년 전 일본에서 독감 치료 중 자살이 뉴스에 나왔는데, 독감이 주로 유행하는 계절적 요인의 문제인지, 아니면 독감 유행이 정말 심할 때 자살이 발생하는 것인지 분석해 보고자 합니다.\n\n3.4.1 실습 데이터\n첫번째 실습 데이터는 감염병 포탈의 인플루엔자 자료입니다. 여기서 다운로드 합니다.\n\n\n\n인플루엔자\n\n\n두번째 실습 자료는 통계청 사망자료 입니다.\n\n\n\n인플루엔자\n\n\n이 둘을 합해 놓은 자료는 아래에 있습니다.\n이것을 data 폴더에 넣겠습니다.\n\nurl &lt;- \"https://raw.githubusercontent.com/jinhaslab/opendata/main/data/flu_suicide0.csv\"\ndownload.file(url, \"data/flu_suicide0.csv\")\n\n\nif(!require(tidyverse)) install.packages('tidyverse')\nif(!require(lubridate)) install.packages('lubridate')\nif(!require(mgcv)) install.packages('mgcv')\nif(!require(dlnm)) install.packages('dlnm')\nif(!require(gam)) install.packages('gam')\nif(!require(forecast)) install.packages('forecast')\nif(!require(Hmisc)) install.packages('Hmisc')\n\n데이터를 살펴보면 ymd 는 숫자 형식의 날짜 (기준 1970년 1월 1일), wsui 는 1주간의 자살 사망자 수, ordweek 는 주중 순위, flu 는 주중 천명당 인플루엔자 환자 수.\n\ndata0 = read_csv(\"data/flu_suicide0.csv\")\n\nNew names:\nRows: 696 Columns: 9\n── Column specification\n────────────────────────────────────────────────────────\nDelimiter: \",\" dbl (8): ...1, ymd, wsui, ordweek, nwd, YR, flu, flu2 date (1):\nymd2\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\nhead(data0)\n\n# A tibble: 6 × 9\n   ...1   ymd  wsui ordweek ymd2         nwd    YR   flu  flu2\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    35 12660   238      35 2004-08-30    36  2004   0.6   0.6\n2    36 12667   211      36 2004-09-06    37  2004   2     2  \n3    37 12674   208      37 2004-09-13    38  2004   2.1   2.1\n4    38 12681   188      38 2004-09-20    39  2004   2.2   2.2\n5    39 12688   213      39 2004-09-27    40  2004   2.5   2.5\n6    40 12695   224      40 2004-10-04    41  2004   2.4   2.4\n\nplot(data0$wsui)\n\n\n\n\n\n\n\n\n신종 플루가 2009년부터 유행했고, 이후 자살자가 관련있다는 뉴스가 나오고 있으니, 2009년 전과 후를 나타내는 변수를 만들겠습니다 .\n\nmyd&lt;-data0 %&gt;% mutate(Change=ifelse(YR&gt;2008, \"from 2009\", \"before 2009\"))\n\n자료가 시계열 자료라는 것을 컴퓨터에게 알려줄 필요가 있습니다. 그리고 싸이클이 있다는 것도요. 우리는 주당 싸이클 (7일 기준)이기 때문에 frequency=365.25/7을 이용하고 시작 날짜를 정해줍니다.\n\ntsui &lt;-ts(myd$wsui, frequency=365.25/7, start = decimal_date(ymd(\"2004-08-30\")))\nlength(myd$wsui)\n\n[1] 696\n\nlength(tsui)\n\n[1] 696\n\nplot(tsui)\n\n\n\n\n\n\n\n\n여기서 시계열적 요소를 찾아 보겠습니다.\n\nd.tsui &lt;-decompose(tsui)\n#d.tsui\nplot(d.tsui) ####### find seasonal and trend\n\n\n\n\n\n\n\nsummary(d.tsui$random)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.     NA's \n-78.1819 -18.8440  -0.3562   1.2124  18.5710 175.0412       51 \n\n\n이번에는 flu에 대한 시계열 분석을 해보겠습니다.\n\nr.tsui &lt;-d.tsui$random # residuals\ns.tsui &lt;-d.tsui$seasonal # seasonal\ntr.tsui &lt;-d.tsui$trend # long term trend\n######### influenza'\ntflu &lt;-ts(myd$flu, frequency=365.25/7, start = decimal_date(ymd(\"2004-08-30\")))\nplot(tflu)\n\n\n\n\n\n\n\n\n이것을 decomposition 하면\n\nd.tflu &lt;-decompose(tflu)\nplot(d.tflu) ####### find seasonal and trend\n\n\n\n\n\n\n\nr.tflu &lt;-d.tflu$random # residuals\ns.tflu &lt;-d.tflu$seasonal # seasonal\ntr.tflu &lt;-d.tflu$trend # long term trend\n\n\n\n3.4.2 simple time-related variable adjusting regression\n결국 resudial flu와 residual sui의 상관관계를 보면 되는 것이라고 생각해 봅시다.\n\nlibrary(Hmisc)\nplot(r.tflu, r.tsui)\nabline(lm(r.tsui ~ r.tflu))\n\n\n\n\n\n\n\nplot(Lag(r.tflu, 3), r.tsui)\nabline(lm(r.tsui ~ Lag(r.tflu, 3)))\n\n\n\n\n\n\n\n\n\npar(mfrow=c(1,2))\nplot(r.tflu, r.tsui, cex=0.2, main=\"no lag model\")\nabline(lm(r.tsui ~ r.tflu))\ntext(15, 130, 'Beta =')\ntext(15, 100, 'P value=')\ntext(30, 130, round(summary(lm(r.tsui ~ Lag(r.tflu, 0)))$coefficients[c(2)], 4))\ntext(30, 100, round(summary(lm(r.tsui ~ Lag(r.tflu, 0)))$coefficients[c(8)], 4))\n\nplot(Lag(r.tflu, 3), r.tsui, cex=0.2, main=\"3 weeks lag model\")\nabline(lm(r.tsui ~ Lag(r.tflu, 3)))\ntext(15, 130, 'Beta =')\ntext(15, 100, 'P value=')\ntext(30, 130, round(summary(lm(r.tsui ~ Lag(r.tflu, 3)))$coefficients[c(2)], 3))\ntext(30, 100, round(summary(lm(r.tsui ~ Lag(r.tflu, 3)))$coefficients[c(8)], 3))\n\n\n\n\n\n\n\n\n\n\n3.4.3 ARIMA\nARIMA 및 시계열 분석 이 과정에서 다룰 수 있는 내용은 다음과 같습니다.\n\n날짜 및 시간 형식 지정\n데이터 전처리 (ts 클래스, 데이터 정리, 결측치 처리, 이상치 처리)\n시계열의 통계적 특징 (자기상관, 정상성, 계절성, 추세)\n\n\n\n\n단계\n\n\n\n\n날짜 및 시간 형식 지정\n\n\n데이터 전처리\n\n\n시계열의 통계적 특징\n\n\n표준 모델\n\n\nARIMA 모델",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>시계열분석</span>"
    ]
  },
  {
    "objectID": "timeseries_1.html#분석-및-예측-회귀-분석",
    "href": "timeseries_1.html#분석-및-예측-회귀-분석",
    "title": "3  시계열분석",
    "section": "3.5 분석 및 예측, 회귀 분석",
    "text": "3.5 분석 및 예측, 회귀 분석\n시계열 분석은 패턴을 찾기 위해 데이터를 분석하는 것이고, 예측은 그 패턴을 미래로 확장하는 것입니다. 시계열 패턴을 이용한 회귀 분석(시계열 회귀 분석)은 시계열 분석을 사용하는 회귀 방법입니다.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>시계열분석</span>"
    ]
  },
  {
    "objectID": "timeseries_1.html#arima-1",
    "href": "timeseries_1.html#arima-1",
    "title": "3  시계열분석",
    "section": "3.6 ARIMA",
    "text": "3.6 ARIMA\nARIMA(자기회귀 통합 이동 평균, Autoregressive Integrated Moving Average)는 문자 그대로 자기회귀와 이동 평균을 이용합니다. 단변량 시계열(univariate time series)로 볼 수 있습니다. ARIMA 모델에서는 여러 매개변수를 자동으로 또는 수동으로 설정하여 구성합니다.\nARIMA(p, d, q)를 이해하면서 가 봅겠습니다. \n\n\n\nparameter\ncontent\nabbr\n\n\n\n\nAR\nAutoregressive part\np\n\n\nI\nIntegrateion, degree of differencing\nd\n\n\nMA\nMoving average part\nq\n\n\n\n위 에서 p, d, q를 찾아 가는 방법을 ARIMA 모델이라고 부를 수 있습니다.\n\nlags 과 forecasting errors로 구분할 수 있습니다.\n\n\n과거의 변수가 현재를 예측, autoregressive part\n\nAR(1) or ARIMA(1,0,0): first order (lag) of AR\nAR(2) or ARIMA(2,0,0): second order (lag) of AR \n\n과거의 error 가 현재를 예측 (forecasting error) = moving average part\n\nMA(1) or ARIMA(0,0,1): first order of MA\nMA(2) or ARIMA(0,0,2): second order of MA\n\n\n\n자기상관관계 부분\n\n\\[ Y_{t} = c + \\Phi_1 Y_{t-1} + \\varepsilon_{t} \\]\n\n\\(t\\) 시간에 관찰되는 변수 (\\(Y_{t}\\))는\n\n상수 (c) 더하기\n바로 1단위 전 변수 (\\(Y_{t-1}\\)) 에 계수(coefficient) (\\(\\Phi\\)) 글 곱한 값을 더하고\n현재의 에러를 \\(t (e_{t})\\)) 더한다\n\n\n\n이동평균 부분\n\n\\[ Y_{t} = c  + \\Theta_1 \\varepsilon_{t-1} + \\varepsilon_t \\]\n\n\\(t\\) 시간에 관찰되는 변수 (\\(Y_{t}\\))는\n\n상수 (c) 더하기\n바로 1단위 전 변수 (\\(\\varepsilon_{t-1}\\)) 에 계수(coefficient) (\\(\\Phi\\)) 글 곱한 값을 더하고\n현재의 에러를 \\(t (e_{t})\\)) 더한다\n\n\n\n결국 자기 상과관계와 이동평균을 한꺼번에 사용하면 아래와 같습니다.\n\n\\[\\begin{align*}\ny_t &= \\phi_1y_{t-1} + \\varepsilon_t\\\\\n&= \\phi_1(\\phi_1y_{t-2} + \\varepsilon_{t-1}) + \\varepsilon_t\\\\\n&= \\phi_1^2y_{t-2} + \\phi_1 \\varepsilon_{t-1} + \\varepsilon_t\\\\\n&= \\phi_1^3y_{t-3} + \\phi_1^2 \\varepsilon_{t-2} + \\phi_1 \\varepsilon_{t-1} + \\varepsilon_t\\\\\n\\end{align*}\\]\n\nd 는 시계열그림에서 ACF, PACF의 형태를 보고 차분의 필요여부 및 차수를 d를 결정하고 AR차수와 MA차수를 결정\n\n어떻게 p, d, q 를 구할수 있을 까요?, 다음 장을 보겠습니다. ** 다음에 기회가 있을 때 하겠습니다.**\n\n3.6.1 arima 감기 자살 , AIC\n아래 ARIMA 모델을 보면 AIC 가 6583정도 나온 것을 알 수 있습니다. 우리는 이것을 통해 AIC가 6583 이하 정도 나오는 gam 모델을 사용하겠다 정도의 개념을 얻었습니다.\n\npar(mfrow=c(1,1))\nauto.arima(myd$wsui)\n\nSeries: myd$wsui \nARIMA(0,1,2) \n\nCoefficients:\n          ma1      ma2\n      -0.3227  -0.0993\ns.e.   0.0379   0.0376\n\nsigma^2 = 756.3:  log likelihood = -3288.64\nAIC=6583.28   AICc=6583.31   BIC=6596.91\n\nmyd &lt;-myd %&gt;% mutate(ma4 =ma(wsui, order=4), ymd2=as.Date(ymd2) ) ### 4weeks moving average\nmyd &lt;-myd %&gt;% mutate(ts.wsui =tsui, ts.ma4=ts(ma4, frequency =365.25/7 ))\nm1&lt;-arima(myd$wsui, order=c(1,1,1), fixed=c(NA, NA)) ## NA means include, 0 means exclude\nm1\n\n\nCall:\narima(x = myd$wsui, order = c(1, 1, 1), fixed = c(NA, NA))\n\nCoefficients:\n         ar1      ma1\n      0.2294  -0.5589\ns.e.  0.0941   0.0797\n\nsigma^2 estimated as 755.2:  log likelihood = -3289.13,  aic = 6584.26\n\ntsdiag(m1)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>시계열분석</span>"
    ]
  },
  {
    "objectID": "timeseries_1.html#aic-bic-in-generalize-additive-model",
    "href": "timeseries_1.html#aic-bic-in-generalize-additive-model",
    "title": "3  시계열분석",
    "section": "3.7 AIC BIC in generalize additive model",
    "text": "3.7 AIC BIC in generalize additive model\n\ngg &lt;-function(x) {\n  model &lt;-glm(data=myd, wsui ~ ns(ymd2, x))\n  aic &lt;-AIC(model)\n  return(aic)\n  }\ngg2 &lt;-function(x) {\n  model &lt;-glm(data=myd, wsui ~ ns(ymd2, x))\n  bic &lt;-BIC(model)\n  return(bic)\n}\n\ntest &lt;-mapply(x=c(50:100), gg);test2&lt;-mapply(x=c(50:100), gg2)\n\npar(mfrow=c(1,2))\nplot(c(50:100), test);plot(c(50:100), test2)\nabline(v=64)\n\n\n\n\n\n\n\n\nAIC는 수렴하지 않아 어렵고, BIC는 64에서 최소 값을 보이네요. 64를 자유도로 선정하고 수행하겠습니다.\n\nmod1&lt;-glm (wsui ~ ns(ymd2, 64), data=myd)\nBIC(mod1)\n\n[1] 6790.573\n\n\nlong term trend (월)과 단기 trend 를 나누어 만들어 보면 어떨까요? 위에 64로 한번에 해결하는 게 더 좋은 모형 같습니다.\n\nmod1&lt;-glm( wsui ~ ns(ordweek, 12)+ns(nwd, 5), data=myd)\nBIC(mod1)\n\n[1] 6907.866\n\n\n기존의 sin cosin 방법으로 시계열 분석을 해보는 것은 어떨까요?\n\npar(mfrow=c(1,1))\nssp&lt;-spectrum(myd$wsui)\n\n\n\n\n\n\n\nper&lt;-1/ssp$freq[ssp$spec==max(ssp$spec)]\nsin.x&lt;-sin(2*pi*myd$ordweek/(365.25/7))\ncos.x&lt;-cos(2*pi*myd$ordweek/(365.25/7))\nmodsean &lt;-glm(wsui ~ ns(sin.x, 2)+ns(cos.x, 2), data=myd)\nmodlgam&lt;-glm(wsui ~ ns(ordweek, 4), data=myd)\n\nplot(myd$ymd2, myd$wsui, ylim=c(-10, 450), col='grey')\npoints(myd$ymd2, modlgam$fitted.values, type='l', col='blue')\npoints(myd$ymd2, modsean$fitted.values, type='l', col='blue')\n\n\n\n\n\n\n\n\n자 이제 2 모델을 검토해 보겠습니다. gam 과 sin cosin 모델 어떤게 더 좋아 보이시나요? 정해진 규칙은 겂습니다.\n\nplot(myd$ymd2, myd$wsui, ylim=c(-10, 450), col='grey')\npoints(myd$ymd2, modlgam$fitted.values, type='l', col='blue')\npoints(myd$ymd2, modsean$fitted.values, type='l', col='blue')\n\nmod1 &lt;-glm(wsui ~ flu+ns(ordweek, 51)+ns(sin.x, 2)+ns(cos.x, 2) , data=myd)\npoints(myd$ymd2, mod1$fitted.values, type='l', col='red')\nmodgam&lt;-glm (wsui ~ ns(ymd2, 64), data=myd)\npoints(myd$ymd2, modgam$fitted.values, type='l',  col='black')\n\n\n\n\n\n\n\n\n정해진 규칙은 없지만 AIC와 BIC로 비교해 볼수 있을 것 같습니다.\n\nAIC(mod1);AIC(modgam)\n\n[1] 6522.708\n\n\n[1] 6490.58\n\nBIC(mod1);BIC(modgam)\n\n[1] 6786.338\n\n\n[1] 6790.573\n\n\n이제 sin과 cos 에 어떠한 df를 주는 것이 좋을 까요?\n\nmyd$econo &lt;- ifelse(myd$YR %in% c(2009), 1, 0)\ngg &lt;-function(x) {\n  model &lt;-glm(data=myd, wsui ~ Lag(flu, 1)+ns(ordweek, 4)+ns(sin.x, x)+ns(cos.x, x))\n  aic &lt;-AIC(model)\n  return(aic)\n}\ngg &lt;-function(x) {\n  model &lt;-glm(data=myd, wsui ~ Lag(flu, 1)+ns(ordweek, 4)+ns(sin.x, x)+ns(cos.x, x))\n  bic &lt;-BIC(model)\n  return(bic)\n}\n\np&lt;-c(1:10)\ntest &lt;-mapply(x=p, gg);test2&lt;-mapply(x=p, gg2)\nplot(p, test)\n\n\n\n\n\n\n\nplot(p, test2)\n\n\n\n\n\n\n\n\n주중 효과 까지 한번 보겠습니다.\n\ngg &lt;-function(x) {\n  model &lt;-glm(data=myd, wsui ~ Lag(flu, 1)+ ns(ordweek, x)+ns(sin.x, 2)+ns(cos.x, 2))\n  aic &lt;-AIC(model)\n  return(aic)\n}\ngg2 &lt;-function(x) {\n  model &lt;-glm(data=myd, wsui ~ Lag(flu, 1)+ns(ordweek, x)+ns(sin.x, 2)+ns(cos.x, 2))\n  bic &lt;-BIC(model)\n  return(bic)\n}\ngg(10)\n\n[1] 6854.818\n\ntest\n\n [1] 7024.330 6937.360 6937.973 6949.360 6961.920 6963.986 6982.745 6988.485\n [9] 6998.475 7012.750\n\np&lt;-c(10:100)\ntest &lt;-mapply(x=p, gg);test2&lt;-mapply(x=p, gg2)\npar(mfrow=c(1,2))\nplot(p, test)\nabline(v=39)\nplot(p, test2)\nabline(v=39)\n\n\n\n\n\n\n\n\n최종 모델은 아래와 같습니다.\n\nmod2 &lt;-glm(data=myd, wsui ~ flu+ ns(ordweek, 39)+ns(sin.x, 2)+ns(cos.x, 2))\npar(mfrow=c(1,1))\nplot(myd$ymd2, myd$wsui, cex=0.5, col='grey', ylim=c(-50, 450))\npoints(myd$ymd2, mod2$fitted.values, type='l', col='red')\n\n\n\n\n\n\n\nBIC(mod2)\n\n[1] 6785.128\n\nAIC(mod2)\n\n[1] 6576.042\n\n\n그럼 이제 lag time 을 non-linear 로 할때 몇차 방정식이 좋을까요? 둘다 2차 방정식이 좋네요\n\ngg3&lt;-function(pp){\n  cb&lt;- crossbasis(myd$flu/10, lag=24, argvar=list(\"lin\"),  arglag = list(fun=\"poly\", degree=pp))\n  model&lt;-glm(data=myd, wsui ~ cb + ns(ordweek, 39)+ns(sin.x, 2)+ns(cos.x, 2))\n  aic&lt;-AIC(model)\n  return(aic)\n}\ngg4&lt;-function(pp){\n  cb1&lt;- crossbasis(myd$flu/10, lag=24, argvar=list(\"lin\"),  arglag = list(fun=\"poly\", degree=pp))\n  model1&lt;-glm(data=myd, wsui ~ cb1 + ns(ordweek, 39)+ns(sin.x, 2)+ns(cos.x, 2))\n  bic&lt;-BIC(model1)\n  return(bic)\n}\np&lt;-c(2:10)\ntest3 &lt;-mapply(pp=p, gg3);test4 &lt;-mapply(pp=p, gg4)\npar(mfrow=c(1,2))\nplot(p, test3)\n\nplot(p, test4)\n\n\n\n\n\n\n\n\n종합해서 나타내 보겠습니다. 이것이 첫번째 종착지 입니다.\n\npar(mfrow=c(1,1))\ncb1&lt;- crossbasis(myd$flu/10, lag=24, argvar=list(\"lin\"),  arglag = list(fun=\"poly\", degree=2))\nmodel1&lt;-glm(data=myd, wsui ~ cb1 + ns(ordweek, 39)+ns(sin.x, 2)+ns(cos.x, 2), family=quasipoisson())\n\npred1.cb1 &lt;-crosspred(cb1, model1, at=1:100, bylag=0.1,  cumul=TRUE)\nplot(pred1.cb1, \"slices\", var=1, col=3, ylab=\"Relative risk of suicide\", #ci.arg=list(density=50, lwd=1),#\n     main=\"Temporal effect by influenza\", \n     xlab=\"Lag (weeks)\",  family=\"A\",#ylim=c(0.980, 1.02),\n     col='black') ;grid()\ntitle(main=\"% increment of influenza like illness\", \n      family=\"A\", \n      adj=1, line=0, font.main=3, cex=0.5 )\n\nlin &lt;-c(5:10)\nabline(v=lin, lty=3, col='lightgray')\naxis(side=1, at=c(6, 7, 8, 9))\n\n\n\n\n\n\n\n\n2009년 이전과 이후를 그려보겠습니다.\n\nmyd2&lt;-myd %&gt;% mutate(sinx=sin.x, cosx=cos.x) %&gt;% mutate(flu210=flu/10)\nmf1d &lt;-myd2 %&gt;% filter(YR &lt;=2008)\nmf2d &lt;-myd2 %&gt;% filter (YR&gt;=2009)\nmf1 &lt;-glm(data=mf1d, wsui ~ flu + ns(ordweek, 25)+ns(sinx, 2)+ns(cosx, 2), family=quasipoisson())\nmf1s&lt;-glm(data=mf1d , flu210 ~ ns(ordweek, 25)+ns(sinx, 2)+ns(cosx,2), family=quasipoisson())\nb2008&lt;-summary(mf1)$coefficient[2,]\nmf2 &lt;-glm(data=mf2d, wsui ~ flu + ns(ordweek, 22)+ns(sinx, 2)+ns(cosx, 2), family=quasipoisson())\nmf2s &lt;-glm(data=mf2d, flu210 ~ns(ordweek, 25)+ns(sinx, 2)+ns(cosx,2), family=quasipoisson())\nf2008&lt;-summary(mf2)$coefficient[2,]\nmfresid&lt;-c(mf1$residuals, mf2$residuals)\nCh &lt;-c(myd2$Change_2008)\n\nWarning: Unknown or uninitialised column: `Change_2008`.\n\n#exp(cbind(\"Relative Risk\"=coef(mf2), confint.default(mf2, level = 0.95)))\n#exp(cbind(\"Relative Risk\"=coef(mf1), confint.default(mf1, level = 0.95)))\n\n\n# E(Y) = intercept + B1X1 +gam(others)\n# E(Y)- intercept - B1X1 = gam(otehrs)\ngamothers1 &lt;- mf1$fitted.values - 0.943684 -(-0.013931) *mf1d$flu210\ngamothers2 &lt;- mf2$fitted.values - 0.8892468 -(0.0023323696) *mf2d$flu210\n# E(Y)- intercept - gam(others)= B1X1  \n# Hence Y axis = E(Y)- intercept - gam(others)\nYaxis.mf1d &lt;- mf1$fitted.values -(0.943684) - gamothers1\nYaxis.mf2d &lt;- mf2$fitted.values -(0.8892468) - gamothers2\n\nmf1d$Yaxis.mf &lt;-Yaxis.mf1d\nmf2d$Yaxis.mf &lt;-Yaxis.mf2d\nplot(mf1d$flu210, Yaxis.mf1d)\n\n\n\n\n\n\n\nplot(mf2d$flu210, Yaxis.mf2d)\n\n\n\n\n\n\n\nplot(myd2$flu210*10, myd2$wsui)\n\n\n\n\n\n\n\n#summary(glm(Yaxis.mf1d ~ mf1d$flu210))\n#summary(glm(Yaxis.mf2d ~ mf2d$flu210))\n\ntt &lt;-c(mf1d$Yaxis.mf, mf2d$Yaxis.mf)\ntt2&lt;-c(mf1$fitted.values, mf2$fitted.values)\nmyd2 &lt;-myd2 %&gt;% mutate(Yaxis.mf =tt, mfresid =mfresid, mf.fit=tt2)\n\n2009년 이후로 좀더 사망하게 되네요.\n\nf3&lt;-ggplot(data=myd2, aes(flu210, Yaxis.mf, col=Change))+geom_line(size=1)+\n  geom_point(data=myd2, aes(flu210, Yaxis.mf, shape=Change), size=0.0)+\n  theme_bw(base_size=14,base_family='Times New Roman')+ \n  theme(panel.border = element_blank(), axis.line = element_line(colour = \"black\"),\n        axis.text.x=element_text(size=12),\n        axis.text.y=element_text(size=12))+\n  xlab(\"Influenza\") +ylab(\"Increment of Suicide\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\nfig3 &lt;-f3 + geom_point(aes(flu210, mfresid, shape=Change), size=3 ) +\n    scale_shape_manual(values=c(1, 20))+ scale_colour_manual(values=c('red', 'grey45'))+ \n     theme(legend.position=\"right\") + \n  labs(title=\"Linear relationship between Influenza and Suicide\",\n       subtitle=\"Beta = -0.066, p = 0.214  before 2009\\n  *RR =  0.013,  p = 0.018     from 2009\") +\n      theme(plot.subtitle=element_text(size=12, hjust=1, face=\"italic\", color=\"black\")) +\n  scale_x_continuous(trans = 'log')\n\nfig3\n\nWarning in scale_x_continuous(trans = \"log\"): log-2.718282 transformation\nintroduced infinite values.\n\n\nWarning in scale_x_continuous(trans = \"log\"): log-2.718282 transformation introduced infinite values.\nlog-2.718282 transformation introduced infinite values.\n\n\n\n\n\n\n\n\n\n지금까지의 내용을 정리해 보겠습니다.\n\nmf11 &lt;-glm(data=mf1d , wsui ~ ns(ordweek, 25)+ns(sinx, 2)+ns(cosx,2))\nmf12 &lt;-glm(data=mf2d, wsui ~ ns(ordweek, 22)+ns(sinx, 2)+ns(cosx, 2))\ntt3&lt;-c(mf11$residuals, mf12$residuals)\nmyd2&lt;-myd2 %&gt;% mutate(f3resid=tt3) %&gt;% mutate(Period=Change)\n\nf1&lt;-ggplot(data=myd2, aes(ymd2, wsui, shape=Change), size=0.3)+ scale_shape_manual(values=c(1, 19), name=\"\")+ \n    geom_point(data=myd2, aes(ymd2, wsui, shape=Change))+\n    #geom_point(aes(x=ymd2, y=f3resid, col=Change)) +\n    geom_line(data=myd2, aes(ymd2, mod2$fitted.values, linetype=\"A\", color='A'))+\n    geom_line(data=myd2, aes(ymd2,flu210*20, linetype=\"B\", color='B'))+\n    geom_line(data=myd2, aes(ymd2, f3resid, linetype=\"C\", color='C'))+\n    \n    scale_linetype_manual(values=c(A=\"dotted\", B=\"solid\", C=\"dashed\"), \n                        labels=c(\"Suicide (Crude)\", \"Influenza like illness\", \"Suicide \\n(Time series adjusted)\"), \n                        name=\"Suicide and Influenza\")+ \n  \n    scale_color_manual(values=c(A=\"black\", B=\"blue\", C=\"red\"), \n                        labels=c(\"Suicide (Crude)\", \"Influenza like illness\", \"Suicide \\n(Time series adjusted)\"), \n                        name=\"Suicide and Influenza\")+ \n  \n  theme(panel.border = element_blank(), axis.line = element_line(colour = \"black\"),\n                                                              axis.text.x=element_text(size=12),\n                                                              axis.text.y=element_text(size=12)) +\n  xlab(\"Years (unit=weeks)\") +ylab(\"Number of weekly suicide\")\n   \n  figure1 &lt;- f1 +\n    geom_smooth(aes(ymd2, f3resid), method='gam', formula=y ~ns(x, 60),\n                se=TRUE, col='red', linetype=\"solid\", size=0.3, fill = 'red')+\n    theme( legend.position =   \"right\") + \n    labs(caption =\"*Beta = weekly suicide number change by % increment of influenza like illness\",\n      title=\"\"#, subtitle=\"Beta = -0.014, p = 0.158  before 2009\\n  Beta =  0.002,  p = 0.011     from 2009\"\n         ) +   theme(plot.title=element_text(size=16, hjust=0.5)) + #face=\"italic\", color=\"black\"))+\n  theme(legend.text=element_text(size=12)) +\n    scale_y_continuous(sec.axis = sec_axis((~./20), name=\"Influenza like illness ( per 100 outpatient )\")) +\n    annotate(\"text\", x = as.Date('2008-09-01'), y = 450, label = 'bold(\"Before 2009 (  )\")', parse=TRUE, family='A', hjust = 1) +\n    annotate(\"text\", x = as.Date('2008-09-01'), y = 430, label = 'italic(\"*Beta = -0.066\")', parse=TRUE, family='A', hjust = 1) + \n    annotate(\"text\", x = as.Date('2008-09-01'), y = 410, label = 'italic(\" p = 0.214\")', parse=TRUE, family='A', hjust = 1) + \n    guides(shape=FALSE)+\n    annotate(\"text\", x = as.Date('2012-01-01'), y = 450, label = 'bold(\"From  2009 (  )\")', parse=TRUE, family='A', hjust = 0) +\n    annotate(\"text\", x = as.Date('2012-01-01'), y = 430, label = 'italic(\"*Beta = 0.013\")', parse=TRUE, family='A', hjust = 0) + \n    annotate(\"text\", x = as.Date('2012-01-01'), y = 410, label = 'italic(\" p = 0.019\")', parse=TRUE, family='A', hjust = 0) +\n    geom_point(x=as.Date('2008-06-25'), y=449, size=3, shape=1) +\n    geom_point(x=as.Date('2013-11-01'), y=449, size=3, shape=19) +\n    geom_vline(xintercept = as.Date('2009-01-01'), linetype=\"dotted\", \n                    color = \"grey50\") #+\n\nWarning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\n\n    #annotate(\"rect\", xmin = as.Date('2004-06-01'), xmax = as.Date('2009-01-01'),  ymin = -50, ymax = 470,\n     #       alpha = .1)\n  \n\n        \n  \n  figure1 \n\n\n\n\n\n\n\n\n여기 까지 실습하시느라 수고하셨습니다. 상기 분석 방법으로 아래 논문을 출판하였습니다. 참고해서 보시면 좋겠습니다. https://journals.plos.org/plosone/article/comments?id=10.1371/journal.pone.0244596",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>시계열분석</span>"
    ]
  },
  {
    "objectID": "Dose_Response_Model.html",
    "href": "Dose_Response_Model.html",
    "title": "4  Dose Response Model",
    "section": "",
    "text": "4.1 Exposure (Dose) and Health\nif(!require(tidyverse)) install.packages(\"tidyverse\")\nif(!require(ggplot2)) install.packages(\"ggplot2\")\nif(!require(knitr)) install.packages(\"knitr\")\nif(!require(kableExtra)) install.packages(\"kableExtra\")\n우리몸에 필수 요소가 있다고 상상해 봅시다. 예를 들어 적혈구 백혈구를 생각해 보는 것입니다. 적혈구가 너무 적으면 빈혈과 같은 질병이 있는 것이고, 너무 많으면 적혈구 과다증이 있어 건강에 해롭습니다.  어떤 필수 요소가 너무 적거나 많은 상태를 질병으로, 적절한 양이 있는 경우 건강상태로 보는 것 입니다. 다음과 같은 상황을 상상해 보겠습니다.\ntrace.e &lt;- seq(1,50, by=0.1)\n#normal range = 15~35\ntrace.e.h=function (x) {\n  ifelse(x&lt;20, 1/(1+exp(-x+10)),\n         ifelse(x&lt;30, rnorm(1, 1/(1+exp(-19)), 0.01), \n                1/(1+exp(-19))-1/(1+exp(40-x))\n                )\n        )\n  }\nhstatus&lt;-trace.e.h(trace.e)+rnorm(length(trace.e), 1, 0.1)\nbasic = tibble(trace.e, hstatus)\n이러한 관계를 그림으로 그려 보겠습니다. 기능적 측면에서 생물학적 필수 요소가 너무 적거나 너무 많으면 사망하거나 질병이 있는 상태로, 절적 수준이 유지되는 것을 정상상태로 볼 수 있습니다.\nbasic %&gt;%\n  ggplot(aes(x= trace.e, y = hstatus)) +\n  scale_x_continuous(name=\"Biological Element\") +\n  scale_y_continuous(name=\"Function\") +\n  theme_minimal()+\n  geom_rect(data=basic[1,],aes(xmin=-Inf, xmax=7 , ymin=-Inf, ymax=Inf), fill= 'grey', alpha=0.8) +\n  geom_rect(data=basic[1,],aes(xmin=7,    xmax=15, ymin=-Inf, ymax=Inf), fill= 'grey', alpha=0.5) +\n  geom_rect(data=basic[1,],aes(xmin=15,   xmax=35, ymin=-Inf, ymax=Inf), fill= 'grey', alpha=0.3) +\n  geom_rect(data=basic[1,],aes(xmin=35,   xmax=43, ymin=-Inf, ymax=Inf), fill= 'grey', alpha=0.5) +\n  geom_rect(data=basic[1,],aes(xmin=43,   xmax=52, ymin=-Inf, ymax=Inf), fill= 'grey', alpha=0.8) +\n  geom_point(size=1, color = 'grey50') +\n  annotate(geom=\"text\", x=c(3,47),  y=c(1.3, 1.3), label=\"Death\",    color=\"red\") +\n  annotate(geom=\"text\", x=c(10,38), y=c(1.5, 1.5), label=\"Disease\",  color=\"black\") +\n  annotate(geom=\"text\", x=25,       y=1.7,         label=\"Normal Function\",color=\"blue\")\n위의 그림을 Y축을 적혈구의 기능 측면에서 본 것으로 상상해보면 이해가 갑니다. 다음에는 적혈구의 가능을 악화시키는 물질에 노출되었다고 상상하고 기능이 아닌 질병 측면에서 볼 수 있습니다. 거꾸로 그래프를 뒤집을 수 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dose Response Model</span>"
    ]
  },
  {
    "objectID": "Dose_Response_Model.html#exposure-dose-and-health",
    "href": "Dose_Response_Model.html#exposure-dose-and-health",
    "title": "4  Dose Response Model",
    "section": "",
    "text": "자료 생성\n\n\n\n그림 그리기",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dose Response Model</span>"
    ]
  },
  {
    "objectID": "Dose_Response_Model.html#실습-1-질병-그림-그리기",
    "href": "Dose_Response_Model.html#실습-1-질병-그림-그리기",
    "title": "4  Dose Response Model",
    "section": "4.2 실습 1: 질병 그림 그리기",
    "text": "4.2 실습 1: 질병 그림 그리기\n\nbasic = basic %&gt;% \n  mutate(disease = -1*hstatus+5, \n         exp.b   = -1*trace.e +50) \nfig1 = basic  %&gt;%\n  ggplot(aes(x= exp.b, y = disease))+\n  theme_minimal()+\n  scale_x_continuous(name=\"Biomarker\") +\n  scale_y_continuous(name=\"Disease\") +\n  geom_rect(data=basic[1,],aes(xmin=-Inf, xmax=7 , ymin=-Inf, ymax=Inf), fill= 'grey', alpha=0.8) +\n  geom_rect(data=basic[1,],aes(xmin=7,    xmax=15, ymin=-Inf, ymax=Inf), fill= 'grey', alpha=0.5) +\n  geom_rect(data=basic[1,],aes(xmin=15,   xmax=35, ymin=-Inf, ymax=Inf), fill= 'grey', alpha=0.3) +\n  geom_rect(data=basic[1,],aes(xmin=35,   xmax=43, ymin=-Inf, ymax=Inf), fill= 'grey', alpha=0.5) +\n  geom_rect(data=basic[1,],aes(xmin=43,   xmax=52, ymin=-Inf, ymax=Inf), fill= 'grey', alpha=0.8) +\n  geom_point(size=1, color = 'orange') +\n  annotate(geom=\"text\", x=c(3,47),  y=c(3.7, 3.7), label=\"Death\",    color=\"red\") +\n  annotate(geom=\"text\", x=c(10,38), y=c(3.5, 3.5), label=\"Disease\",  color=\"black\") +\n  annotate(geom=\"text\", x=25,       y=3.3,         label=\"Normal Function\",color=\"blue\") \nfig1",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dose Response Model</span>"
    ]
  },
  {
    "objectID": "Dose_Response_Model.html#지역사회-연구-community-base-cohort-study",
    "href": "Dose_Response_Model.html#지역사회-연구-community-base-cohort-study",
    "title": "4  Dose Response Model",
    "section": "4.3 지역사회 연구 (community base cohort study)",
    "text": "4.3 지역사회 연구 (community base cohort study)\n지역사회 연구에서는 질병이 있는 사람 또는 기능이 약화된 사람은 병원에 입원해 있는 등 사회생활이 어려우므로 참여하지 못할 수 있습니다. 따라서 장기간 추적 관찰을 하지 않는 경우 바이오마커와 질병간에 U-shap 을 보이게 됩니다. 장기 관찰을 하거나 충분한 관찰을 하면 질병이 새로 생기는 부분을 찾을 수 있으므로 J-shap으로 보일 수 도 있습니다. 우리가 과거력이 있는 사람 또는 적절한 방법으로 건강이 악화되어 있는 사람을 제외하는 경우 바이오마커와 질병의 선형적 관계를 관찰할 수 있는 경우 입니다.\n\nfig1 +\n  annotate(geom=\"text\", x=c(7, 43),  y=c(3.3, 3.3), label=\"Hospital (selection bias)\",  color=\"grey50\") +\n  annotate(geom=\"text\", x=c(10),  y=c(3.8), label=\"Short term follow up\",  color=\"purple\") +\n  geom_segment(aes(x=10, xend=40, y=3.7, yend=3.7), size = 0.5, color='grey50',\n               arrow = arrow(length = unit(0.2, \"cm\"), type = \"closed\")) +\n  annotate(geom=\"text\", x=c(30),  y=c(4.1), label=\"Long term follow up\",  color=\"purple\") +\n  geom_segment(aes(x=10, xend=50, y=4, yend=4), size = 0.5, color='grey50',\n               arrow = arrow(length = unit(0.2, \"cm\"), type = \"closed\"))\n\n\n\n\n\n\n\n\n  산업보건에서는 건강한 근로자가 직장을 갖고 직장을 갖은 후에 일에 따라 물리화학적 인자에 노출이 됩니다. 만약 위의 그림에서 바이오마커가 일을하면서 노출되는 유해인자와 관련이 있다면, 사업장에서는 바이오마커가 매우 낮은 사람은 없을 것입니다. 그리고 유해인자에 노출이 많이 되어 질병이 생기고 병원에 가게된다면, 사업장을 중심으로 연구하는 경우 연구대상에 참여하지 못하게 됩니다. 즉 위의 그림에서 short-term follow up 의 상황이 발생하게 됩니다. 그런데 장기간 관찰하고 퇴사후의 자료도 이용한다면 long term follow up 과 같이 가게됩니다. 이때 상관 분석을 수행하면 short-term follow up에서는 U-shap으로, long term follow up 에서는 J-shap 으로 나타나게 됩니다. 실제 연구에서도 비슷한 상황이 발생하기도 합니다. 이때 우리가 얻은 데이터가 무엇을 목적으로 어떠한 설계로 만들어 졌는지를 관찰하고 정말 질병이 생길만한 사람을 제외한 현장에서 연구를 수행하고 있는 것은 아닌지 고민해 보아야 합니다.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dose Response Model</span>"
    ]
  },
  {
    "objectID": "Dose_Response_Model.html#모형-차이-sigmoid-curve-vs-linear-regression",
    "href": "Dose_Response_Model.html#모형-차이-sigmoid-curve-vs-linear-regression",
    "title": "4  Dose Response Model",
    "section": "4.4 모형 차이: sigmoid curve vs linear regression",
    "text": "4.4 모형 차이: sigmoid curve vs linear regression\n  위의 그래프를 절반을 나누어서 적절한 기능을 하고 있는 사람만을 대상으로 장기간 추적관찰했다고 가정해 보겠습니다. 그러면 바이오마커와 질병의 관계를 선형으로 예측할 수도 있습니다. 노출이 지속되더라도 건강의 악화는 어느정도 포화될수 있으므로 (모두 다 계속 사망하지는 않으므로) 노출의 크기와 질병의 관계는 sigmoid curve 관계가 있을 수 있습니다.\n  어떤게 좋을까요? 정답은 없지만 LD50을 고려해서 생각해 보겠습니다. LD50이란 노출된 사람 중의 50%가 사망하는 농도를 의미합니다. 즉 LD50가 큰 물질은 적은 물질보다 많이 노출되어야 노출된 사람 중의 50%가 사망하므로 더 안전한 물질입니다.\n\nsigmoid.f = function(x){\n  1/(1+exp(5-x))\n}\n\ndf = tibble(\ndose.e = c(1:10),  \nresp   = sigmoid.f(dose.e)\n)\ndf %&gt;% kbl() %&gt;%\n  kable_paper(\"hover\", full_width = F)\n\n\n\n\ndose.e\nresp\n\n\n\n\n1\n0.0179862\n\n\n2\n0.0474259\n\n\n3\n0.1192029\n\n\n4\n0.2689414\n\n\n5\n0.5000000\n\n\n6\n0.7310586\n\n\n7\n0.8807971\n\n\n8\n0.9525741\n\n\n9\n0.9820138\n\n\n10\n0.9933071\n\n\n\n\n\n\nplot(df)\n\n\n\n\n\n\n\n\nLD50은 설명을 했고, 과도한 비교를 위해서 LD70을 보고 이야기 해보겠습니다.\n\nset.seed(50)\nx&lt;-seq(0, 10, 0.01)\ny&lt;-sigmoid.f(x)+rnorm(length(x), mean=0, sd=0.1)\npb&lt;-c(rnorm(500, 0, 0.001), rnorm(300, 0, 0.01), rnorm(100, 0.1, 0.05),rnorm(101, -0.1, 0.05))\nresp = y + pb\nld70.sm = x[which( sigmoid.f(x) &gt;0.69 & sigmoid.f(x) &lt; 0.71)] %&gt;% min(.)\nld70.sm\n\n[1] 5.81\n\nmod1&lt;-glm(resp ~ poly(x, 1))\npred1&lt;-predict(mod1)\nld70.lm = x[which(pred1 &gt;0.69 & pred1 &lt;0.71)] %&gt;% min(.)\nld70.lm\n\n[1] 6.47\n\ndf = tibble(dose = x, response= resp)\ndf %&gt;%\n  ggplot(aes(x=dose, y=response)) +\n  geom_point(color = 'grey80') + theme_minimal()+\n  geom_line(aes(y= sigmoid.f(dose)), color ='red') +\n  geom_line(aes(y= predict(lm(response ~ poly(dose, 1)))), color ='blue') +\n  geom_vline(xintercept =   5, linetype=2, color='grey50') +\n  geom_hline(yintercept = 0.7, linetype=2, color='grey50') +\n  annotate(geom=\"text\", x=ld70.sm -1,  y=0.9, label=\"LD70\",  \n           color=\"purple\", hjust=1) +\n  geom_segment(aes(x=ld70.sm -1, xend=ld70.sm, y=0.85, yend=0.7), size = 0.1, color='grey30',\n               arrow = arrow(length = unit(0.2, \"cm\"), type = \"closed\")) +\n  annotate(geom=\"text\", x=ld70.lm +1,  y=0.0, label=\"LD70 by linear assumption\",  \n           color=\"purple\", hjust=0) +\n  geom_segment(aes(x=ld70.lm +1, xend=ld70.lm, y=0.05, yend=0.7), size = 0.1, color='grey30',\n               arrow = arrow(length = unit(0.2, \"cm\"), type = \"closed\"))\n\n\n\n\n\n\n\n\n가상의 자료입니다. 그래도 중요하게 기억할 점은 선형과 비선형 커브로 할 경우 LD70이 달라지고, 독성에 대한 설명이 달라집니다. 따라서 어떠한 방식으로 설명할지 꼭 고민해야 합니다.  저 농도나 고농도에서는 더 큰 차이가 나타납니다. 그런데 대 부분 저농도의 노동자 들이나 고농도 노출의 노동자가 연구에 참여하기 어려운 상황이 발생합니다. 앞서 계속해서 이야기하는 건강근로자 효과 등을 상기 시켜 봅시다.  그럼 어떤 모델이 가장 적당할 까요? 우선 모델 적합도를 설명도로 비교해 볼 수 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dose Response Model</span>"
    ]
  },
  {
    "objectID": "Dose_Response_Model.html#코호트-특성에-따른-상황",
    "href": "Dose_Response_Model.html#코호트-특성에-따른-상황",
    "title": "4  Dose Response Model",
    "section": "4.5 코호트 특성에 따른 상황",
    "text": "4.5 코호트 특성에 따른 상황\n첫 수업 시간에 이야기한 것 처럼, 처음에는 질병이 생긴 노동자 위주로 연구가 진행되게 됩니다. 따라서 고농도 노출자 이면서 질병이 있는 사람으로 구성된 데이터에서는 상대적으로 높은 농도에서 질병이 발생하는 연구 결과과 발표 되기도 합니다. 그리고 그림에서 보듯이 선형관계를 고민하지 않는 다면 어디를 기준으로 해야할지 알 수 없는 상태입니다.  결론적으로 위험하다 는 알수 있지만, 얼마나 위험하다는 아직 연구가 되지 않은 상태라는 것을 기억해야 합니다.\n\nearly_cohort = df %&gt;% filter(dose &gt; 5)\n\ndf %&gt;%\n  ggplot(aes(x=dose, y=response)) +\n  geom_point(color = 'grey80') + theme_minimal()+\n  geom_line(aes(y= sigmoid.f(dose)), color ='red') +\n  geom_line(aes(y= predict(lm(response ~ dose))), color ='blue') +\n  geom_vline(xintercept =   5, linetype=2, color='grey50') +\n  geom_hline(yintercept = 0.7, linetype=2, color='grey50') +\n  ## add 1 \n  annotate(geom=\"text\", x=c(7.5),  y=c(-0.2), \n           label=\"Early cohort (hospital base)\",  color=\"purple\", hjust=0) +\n  geom_rect(data=df[1,],aes(xmin=-Inf,   xmax=5, ymin=-Inf, ymax=Inf), \n            fill= 'grey', alpha=0.6) +\n  geom_line(data= early_cohort,\n              aes(y= predict(lm(response ~ poly(dose,3)))), color ='orange', size = 2) +\n   annotate(geom=\"text\", x=ld70.sm -1,  y=0.9, label=\"LD70\",  \n           color=\"purple\", hjust=1) +\n  geom_segment(aes(x=ld70.sm -1, xend=ld70.sm, y=0.85, yend=0.7), size = 0.1, color='grey30',\n               arrow = arrow(length = unit(0.2, \"cm\"), type = \"closed\")) +\n  annotate(geom=\"text\", x=ld70.lm +1,  y=0.0, label=\"LD70 by linear assumption\",  \n           color=\"purple\", hjust=0) +\n  geom_segment(aes(x=ld70.lm +1, xend=ld70.lm, y=0.05, yend=0.7), size = 0.1, color='grey30',\n               arrow = arrow(length = unit(0.2, \"cm\"), type = \"closed\"))",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dose Response Model</span>"
    ]
  },
  {
    "objectID": "Dose_Response_Model.html#코호트-특성에-따른-상황과-이론적-모델",
    "href": "Dose_Response_Model.html#코호트-특성에-따른-상황과-이론적-모델",
    "title": "4  Dose Response Model",
    "section": "4.6 코호트 특성에 따른 상황과 이론적 모델",
    "text": "4.6 코호트 특성에 따른 상황과 이론적 모델\n어느정도 잘 갖추어진 코호트를 생각해 보겠습니다. Dose 2 부터 노출된 사람을 대상으로 하였다고 가정하겠습니다.\n\n# subcohort 2nd phase (1st phase is hospital base cohort)\ns2c &lt;- df %&gt;% filter(dose &gt;2)\ns2m1&lt;-lm(data=s2c, response ~ poly(dose, 1))\ns2m2&lt;-lm(data=s2c, response ~ poly(dose, 2))\ns2m3&lt;-lm(data=s2c, response ~ poly(dose, 3))\ns2m4&lt;-lm(data=s2c, response ~ poly(dose, 4))\n\ndf %&gt;%\n  ggplot(aes(x=dose, y=response)) +\n  geom_point(color = 'grey80') + theme_minimal()+\n  geom_point(data=s2c, color = 'grey40', size =2   ) +\n  geom_line(data=s2c, aes(y=predict(s2m1)), color = 'chocolate1', size = 1) +\n  geom_line(data=s2c, aes(y=predict(s2m2)), color = 'chartreuse1', size = 1) +\n  geom_line(data=s2c, aes(y=predict(s2m3)), color = 'cadetblue1', size = 1) +\n  geom_line(data=s2c, aes(y=predict(s2m4)), color = 'deeppink1', size = 1) \n\n\n\n\n\n\n\n\n모델로 보면 선형 예측이 모형 적합도가 가장 낮다(low)고 나타나고 차수가 높을 수록 좋다고 나타나고 있습니다. 그런데, 어떤게 더 맞을 까요?.\n\nanova(s2m1, s2m2, s2m3, s2m4) \n\nAnalysis of Variance Table\n\nModel 1: response ~ poly(dose, 1)\nModel 2: response ~ poly(dose, 2)\nModel 3: response ~ poly(dose, 3)\nModel 4: response ~ poly(dose, 4)\n  Res.Df     RSS Df Sum of Sq        F  Pr(&gt;F)    \n1    798 18.9099                                  \n2    797 13.4734  1    5.4365 440.8574 &lt; 2e-16 ***\n3    796  9.8806  1    3.5928 291.3455 &lt; 2e-16 ***\n4    795  9.8036  1    0.0770   6.2423 0.01267 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n만약 농도가 높을 때 낮아지고 있는 부분을 고려한다면 어떻게 될까요? (실제로도 농도가 높은 곳에 근무하는 노동자는 만성 질병이 일어나기 전에 손상으로 사망하는 연구가 있습니다. ) 따라서 그런 산업보건적 특성을 고려하면 어떻게 될 까요? 단순하게 9 이상 농도를 고려하지 않도록 하겠습니다.\n\ns3c &lt;- df %&gt;% filter(dose &gt;2) %&gt;% filter(dose &lt;9)\ns3m1&lt;-lm(data=s3c, response ~ poly(dose, 1))\ns3m2&lt;-lm(data=s3c, response ~ poly(dose, 2))\ns3m3&lt;-lm(data=s3c, response ~ poly(dose, 3))\ns3m4&lt;-lm(data=s3c, response ~ poly(dose, 4))\n\ndf %&gt;%\n  ggplot(aes(x=dose, y=response)) +\n  geom_point(color = 'grey80') + theme_minimal()+\n  geom_point(data=s3c, color = 'grey40', size =2   ) +\n  #geom_line(data=s2c, aes(y=predict(s2m1)), color = 'grey30', size = 1) +\n  geom_line(data=s2c, aes(y=predict(s2m4)), color = 'grey30', size = 1) +\n  #geom_line(data=s3c, aes(y=predict(s3m1)), color = 'orange', size = 1) \n  geom_line(data=s3c, aes(y=predict(s3m4)), color = 'orange', size = 1) \n\n\n\n\n\n\n\ndf %&gt;%\n  ggplot(aes(x=dose, y=response)) +\n  geom_point(color = 'grey80') + theme_minimal()+\n  geom_point(data=s3c, color = 'grey40', size =2   ) +\n  geom_line(data=s2c, aes(y=predict(s2m1)), color = 'grey30', size = 1) +\n  #geom_line(data=s2c, aes(y=predict(s2m4)), color = 'grey30', size = 1) +\n  geom_line(data=s3c, aes(y=predict(s3m1)), color = 'orange', size = 1) \n\n\n\n\n\n\n\n  #geom_line(data=s3c, aes(y=predict(s3m4)), color = 'orange', size = 1) \n\n3차 이상의 모형에서는 큰 차이가 나지 않지만, 선형 모형에서는 차이가 상당합니다. 어떤 것이 더 맞다는 것은 아직 논할 단계는 아니고, 차이가 있다는 것을 기억하면 좋겠습니다. 그래서 실제 보고하고 적용할 때 현장에 더 적합한 것이 무엇인지, 목적이 보호 인지, 보상인지 등을 고려하여 해야 겠습니다.\n\nlibrary(gam)\n######전체 자료 실습\ns4c = df %&gt;% filter(dose &lt;9)\ns4m1&lt;-lm(data=s4c, response ~ poly(dose, 1))\ns4m3&lt;-lm(data=s4c, response ~ poly(dose, 3))\ns4ms&lt;-lm(data=s4c, response ~ sigmoid.f(dose-5))\ns4mg&lt;-gam(data=s4c, response ~ s(dose, 20))\nanova(s4m1, s4m3, s4ms, s4mg)\n\nAnalysis of Variance Table\n\nModel 1: response ~ poly(dose, 1)\nModel 2: response ~ poly(dose, 3)\nModel 3: response ~ sigmoid.f(dose - 5)\nModel 4: response ~ s(dose, 20)\n  Res.Df    RSS     Df Sum of Sq       F    Pr(&gt;F)    \n1    898 14.876                                       \n2    896 10.064  2.000     4.812  233.61 &lt; 2.2e-16 ***\n3    898 67.272 -2.000   -57.208 2777.26 &lt; 2.2e-16 ***\n4    879  9.053 18.999    58.219  297.52 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ndf %&gt;%\n  ggplot(aes(x=dose, y=response)) +\n  geom_point(color = 'grey80') + theme_minimal()+\n  geom_point(data=s4c, color = 'grey40', size =2   ) +\n  geom_line(data=s4c, aes(y=predict(s4m1)), color = 'grey10', size = 1) +\n  geom_line(data=s4c, aes(y=predict(s4m3)), color = 'grey30', size = 1) +\n  geom_line(data=s4c, aes(y=predict(s4ms)), color = 'deepskyblue', size = 1) +\n  geom_line(data=s4c, aes(y=predict(s4mg)), color = 'orange', size = 1) \n\n\n\n\n\n\n\n\n\n4.6.1 Take home message\n\n고 노출 집단:\n\n\n높은 표준 사망비(SMR)에도 불구하고 용량 증가에 따른 질병 발생 증가 경향이 명확하지 않을 수 있습니다. 즉, 선형적인 용량-반응 관계가 나타나지 않을 수 있습니다.\n\n\n중간 용량 노출 집단:\n\n\n용량-반응 관계가 관찰되지만, 건강한 노동자 효과로 인해 실제 관계가 약화될 수 있습니다. 즉, 실제보다 용량과 질병 발생 간의 연관성이 낮게 나타날 수 있습니다.\n\n\n전체 용량 노출 집단:\n\n\n용량-반응 관계를 분석하고, 건강한 노동자 효과를 통제하여 더욱 정확한 결과를 얻을 수 있습니다.\n모델 선택 과정에서 LD50(반수 치사량)의 과대 또는 과소 평가가 발생할 수 있으므로 주의해야 합니다.\n최적의 모델이라 하더라도, 건강한 노동자 효과로 인한 데이터 편향 때문에 실제 질병 발생 양상을 완벽하게 반영하지 못할 수 있습니다.\n\n\n권고 사항: 기존 모델에 의존하기보다는, 연구 특성에 맞는 자체적인 용량-반응 모델을 개발하여 적용하는 것이 바람직합니다.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dose Response Model</span>"
    ]
  },
  {
    "objectID": "Dose_Response_Model.html#threshold-찾기-change-point-찾기",
    "href": "Dose_Response_Model.html#threshold-찾기-change-point-찾기",
    "title": "4  Dose Response Model",
    "section": "4.7 Threshold 찾기 (change point 찾기)",
    "text": "4.7 Threshold 찾기 (change point 찾기)\nthreshold를 찾는 방법 중에 threshold point마다, piecewise regression을 반복해서 구하고, 최적의 모델을 찾는 방법을 사용할 수 있습니다. piecewise regression 의 간단한 설명은 다음과 같습니다\n\n\n\n\n\n\n\nthreshold points (piecewise regression)\ncodes\n\n\n\n\ntotal\nResp = α + β1 · Dose + β2 ·( Dose – Ɵ) + + Ɛ0\n\n\nIf Dose &lt; Ɵ\nResp= α + β1 · Dose + Ɛ0\n\n\nIf Dose &gt; Ɵ\nResp= α - β2 ·Ɵ +( β1 + β2 )· Dose + Ɛ0\n\n\nmodel selection\nminimal AIC value\n\n\n\n\n4.7.1 자료 생성\n아래 처럼 임로 데이터를 생성해 보았습니다. 어떤 유해물질 노출 (Dose) 에 따라 건강영향 (Resp) 가 나타났다고 생각해 보겠습니다. 그리고 일정량에서 threshold가 있다고 생각해보겠습니다.\n\nset.seed(0)\ndose &lt;- seq(0,10, 0.1)\nlength(dose)\n\n[1] 101\n\npb&lt;-c(rnorm(50, 0, 0.001), rnorm(30, 0, 0.01), rnorm(10, 0.1, 0.05),rnorm(11, -0.1, 0.05))\nresp &lt;-1/(1+exp(-(dose-5)))+rnorm(length(dose), 0, 0.1)+pb\n\nplot(dose, resp, xlab='Dose', ylab='Response', cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)\n\n\n\n\n\n\n\ncohort&lt;-data.frame(dose, resp, pb)\n\n\n\n4.7.2 가상의 threshold 값 수행해보기\n한 1에서 5 사이에 있어 보입니다. 이를 통해 예상되는 matrix (outdata)를 구해보았습니다. outdata의 행의 이름을 threshold point에 따라 intercept, beta for before threshold, and its p value, beta for post threshold and its pvalue와 그때은 AIC 값을 구해보겠습니다 . 우선 하나만 구해보겠습니다. therhold가 1일때와 5일때를 를 가정해 보겠습니다.\n\ncpdose &lt;- ifelse(dose -1 &gt;0, dose -1, 0)\ncpm &lt;- glm(resp ~ dose + cpdose)\nsummary(cpm)$aic\n\n[1] -92.20184\n\n\n\ncpdose &lt;- ifelse(dose -5 &gt;0, dose -5, 0)\ncpm &lt;- glm(resp ~ dose + cpdose)\nsummary(cpm)$aic\n\n[1] -86.9744\n\n\n어떤 가정이 모델 적합도를 높이나요? 네 threshold가 1일 때 입니다. 그럼 2랑도, 2.5랑도 비교해 봐야겠지요. 이때 반복 분석을 수행해보도록 하겠습니다.\n위의 모델을 함수로 만들었습니다\n\nthr_fun &lt;- function(thres){\ncpdose &lt;- ifelse(dose - thres &gt;0, dose - thres, 0)\ncpm    &lt;- glm(resp ~ dose + cpdose)\naic    &lt;- summary(cpm)$aic\ndata.frame(\n  'threshold' = thres,\n  'aic'       = aic)\n}\n\n이것을 돌릴 범위를 정해보겠습니다.\n\n# 이게 어떤 의미 일까요?\ndose[which(dose == 1):which(dose == 5)]\n\n [1] 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 2.1 2.2 2.3 2.4 2.5 2.6 2.7 2.8\n[20] 2.9 3.0 3.1 3.2 3.3 3.4 3.5 3.6 3.7 3.8 3.9 4.0 4.1 4.2 4.3 4.4 4.5 4.6 4.7\n[39] 4.8 4.9 5.0\n\n\n이제 반복해서 작업해 보겠습니다.\n\nsimul_list &lt;- list()\nsimul_list &lt;- lapply(dose[which(dose ==1):which(dose ==5)],                     thr_fun\n                     )\n\n이제 데이터 프레임으로 만들어 보겠습니다.\n\nsimul_dat &lt;- do.call(rbind, simul_list)\n\n그림을 그려보겠습니다.\n\nlibrary(ggplot2)\nopt.thres &lt;- simul_dat$threshold[which.min(simul_dat$aic)]\n\nsimul_dat %&gt;%\n  ggplot(aes(x = threshold, y = aic)) +\n  geom_line() +\n  geom_vline(xintercept = opt.thres) +\n  geom_text(x = opt.thres + 0.8, y = -90, color = 'red', \n            label = paste0(round(opt.thres, 3), '점에서 최소 AIC를 보입니다.' )) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n즉 2.1에서 threshold를 잡아 모델을 그리면 가장 적합함을 알 수 있습니다.\n\nthres = 2.1\nf_cpdose &lt;- ifelse(dose - thres &gt;0, dose - thres, 0)\nf_cpm    &lt;- glm(resp ~ dose + f_cpdose)\n\n\nprepwlm &lt;- predict(f_cpm)\nscaleFUN &lt;- function(x) sprintf(\"%.2f\", x)\ncohort %&gt;%\n  ggplot(aes(x= dose, y = resp)) +\n  geom_point() +\n  theme_minimal() +\n  scale_y_continuous(labels = scaleFUN) +\n  geom_line(aes(y = prepwlm), color ='red')\n\n\n\n\n\n\n\n\n만약 threshold 전에는 질병이 생기지 않는다고 가정하면 어떻게 될가요? dose 대신 predose 를 넣어 주면 됩니다.\n\nthres = 2.1\nf_cpdose &lt;- ifelse(dose - thres &gt;=0, dose - thres, 0)\nf_predose &lt;- ifelse(dose - thres &lt;=0, 0, dose - thres )\nf_cpm    &lt;- glm(resp ~ f_predose + f_cpdose)\n\n\nprepwlm &lt;- predict(f_cpm)\nscaleFUN &lt;- function(x) sprintf(\"%.2f\", x)\ncohort %&gt;%\n  ggplot(aes(x= dose, y = resp)) +\n  geom_point() +\n  theme_minimal() +\n  scale_y_continuous(labels = scaleFUN) +\n  geom_line(aes(y = prepwlm), color ='red')",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dose Response Model</span>"
    ]
  }
]